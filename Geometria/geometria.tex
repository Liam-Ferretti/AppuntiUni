\documentclass[a4paper, 12pt, openany]{book}

% pacchetti comuni
\input{../preambolo.tex}
\newcommand{\rel}[1][R]{R}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}

\numberwithin{equation}{chapter}

% info documento
\title{Appunti di Geometria}
\author{Liam Ferretti}

\date{\today}

\begin{document}
	
	\maketitle
	
	\chapter*{Sommario del corso}
		Le informazioni sul corso si trovano sul \hyperlink{https://sites.google.com/uniroma1.it/kieranogrady}{sito del docente}.
		
		Di regola il lunedì verranno svolti esercizi o chiariti dubbi, e le lezioni saranno svolte da S. Molcho.
		
		Ogni settimana (probabilmente il giovedì) verranno caricati degli esercizi su classroom da riconsegnare entro domenica sera.
		
		Il ricevimento avrà luogo nello studio 137 nell'edificio CU006 il martedì dalle 11:15 alle 12:45.
		
		Le dispense sono disponibili sul sito, il libro non è necessario.
		
	\tableofcontents
	\clearpage
	
	\chapter{Insiemi}
	Per insieme si intende una collezione di oggetti, detti elementi.
	Preso l'insieme $X$ e $a$ un elemento, allora:
	\[
	a \in X: \text{ significa che "a è un elemento di X"}
	\]
	
	\[
	a \not \in X: \text{ significa che "a NON è un elemento di X"}
	\]
	
	Per definire un insieme si usa questa notazione:
	\[
	X := \{a | a \text{ ha la proprietà P}\}
	\]
	
	Es: 
	\[
		X_a := \{a \in \mathbb{N} \mid 2 | a\} = \{0, 2, 4, 6, 8, ...\}
	\]
	Con $2 \mid a$ si intende che 2 è un divisore di a, quindi che a è pari.\newline
	
	Esiste un insieme chiamato insieme vuoto che non contiene nessun elemento ed è rappresentato con: $\varnothing$ \newline
	
	È possibile dichiarare una famiglia di insiemi numerati da un altro insieme in questo modo:
	\[
	\{X_i\}_{i \in I}
	\]
	
	Es: 
	\[
	X_a := \{m \in \mathbb{Z} \mid a | m\}
	\]
	Allora:
	\begin{align*}
		X_0 &:= \{0\} \\
		X_1 &:= \mathbb{Z} \\
		X_2 &:= \{0, \pm 2, \pm 4, ...\}
	\end{align*}
	\newline	
	Insiemi che è necessario conoscere:
	\[
		\mathbb{N} = \{ \text{numeri naturali} \}
	\]
	\[
		\mathbb{Z} = \{ \text{numeri interi} \}
	\]
	\[
		\mathbb{Q} = \{ \text{numeri razionali} \} = \left\{ \frac{p}{q} \;\middle|\; p,q \in \mathbb{Z}, \; q \neq 0 \right\}
	\]
	\[
		\mathbb{R} = \{ \text{numeri reali} \}
	\]
	\[
		\mathbb{C} = \{ \text{numeri complessi} \}
	\]
	
	\section{Sotto insieme}
	Presi due insiemi $X, Y$, $X$ è sotto insieme di $Y$, se ogni elemento di X è elemento di Y, formalmente si esprime con:
	\[
	X \subset Y \iff \forall x \in X, x \in Y
	\]
	
	OSS: X è sotto insieme di se stessa in quanto contiene tutti i suoi elementi, quindi ha senso dire che:
	\[
	X \subset X
	\]
	
	\section{Operazioni tra insiemi}
	Le operazioni che si possono effettuare tra insiemi sono diverse:
	\begin{itemize}
		\item Unione, rappresentata da $\cup$
		\item Intersezione, rappresentata da $\cap$
		\item Prodotto cartesiano, rappresentata da $\times$
	\end{itemize}
	
	\subsection{Unione}
	Preso l'insieme:
	 
	\[
	X_i := \{m \in \mathbb{Z} \tc i | m\}
	\]
	L'unione degli insiemi $X_i$ è l'insieme $X \tc x\in X \iff \exists i \in I \tc x \in X_i$
	
	\[
	X = \bigcup_{i \in I} X_i
	\]
	
	Se $I = \{1, 2\} \rightarrow X_1 \cup X_2$
	
	Se $I = \{1, 2, 3\} \rightarrow X_1 \cup X_2 \cup X_3$
		
	Se $\displaystyle I = \mathbb{Z} \rightarrow \bigcup_{i \in I} X_i = \mathbb{Z}$
	
	\subsection{Intersezione}
	Preso l'insieme:
	
	\[
	X_i := \{m \in \mathbb{Z} \tc i | m\}
	\]
	L'intersezione degli insiemi $X_i$ è l'insieme $X \tc x\in X \iff \forall i \in I, \exists x \in X_i$
	
	\[
	X = \bigcap_{i \in I} X_i
	\]
	
	Se $I = \{1, 2\} \rightarrow X_1 \cap X_2$
	
	Se $I = \{1, 2, 3\} \rightarrow X_1 \cap X_2 \cap X_3$
	
	Se $\displaystyle I = \mathbb{Z} \rightarrow \bigcap_{i \in I} X_i = \{0\}$	
	
	\subsection{Prodotto cartesiano}
	Il prodotto cartesiano di due insiemi $X, Y$, è definito come l'insieme i cui elementi sono le coppie ordinate $(x,y)$, con $x\in X, y\in Y$.
	\[
	X = Y = \mathbb{R} \rightarrow \mathbb{R} \times \mathbb{R} := \{(x, y) \tc x,y\in \mathbb{R}\}
	\]
	\[
	\mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}^2
	\]
	Se si hanno più insiemi:
	\[
	X_1 \times X_2 \times X_3 \times ... \times X_n := \{(x_1, x_2, x_3, ... ,x_n) \tc x_n \in X_n¨\}
	\]
	
	\chapter{Applicazione tra insiemi}
	
	Presi gli insiemi $X, Y$, si definisce un'applicazione $f$ da $X$ (insieme di input o \textbf{dominio}) a $Y$ (insieme di output o \textbf{codominio}) come una legge che associa a ogni elemento $x \in X$ un elemento $f(x) \in Y$. La notazione è:
	\[
	X \xlongrightarrow{f} Y
	\]
	ovvero, l'applicazione manda dall'insieme $X$ all'insieme $Y$.  
	Se si considerano i singoli elementi degli insiemi, si scrive:
	\[
	x \mapsto f(x)
	\]

	Es:
	\begin{align*}
		\mathbb{Z} & \xlongrightarrow{f} \mathbb{Z} \\
		m & \longmapsto 3m
	\end{align*}
	
	Affinché 2 applicazioni sono uguali devono coincidere: \textbf{dominio}, \textbf{codominio} e \textbf{funzione}.
	
	\section{Composizione di applicazioni}
	Presi gli insiemi $X$, $Y$, $Z$ e le applicazioni $f$ e $g$ allora:
	
	\[
	\begin{tikzpicture}[>=stealth, node distance=2cm]
		\node (X) {$X$};
		\node (Y) [right of=X] {$Y$};
		\node (Z) [right of=Y] {$Z$};
		
		\draw[->] (X) -- node[above] {$g$} (Y);
		\draw[->] (Y) -- node[above] {$f$} (Z);
		
		\node (x) [below of=X] {$x$};
		\node (gx) [below of=Y] {$g(x)$};
		\node (fgx) [below of=Z] {$f(g(x))$};
		
		\draw[|-{Stealth}] (x) -- (gx);
		\draw[|-{Stealth}] (gx) -- (fgx);
		
	\end{tikzpicture}
	\]
	è possibile definire la composizione di g e f, ovvero una applicazione del tipo:
	
	\[
	X \xrightarrow{f \circ g} Z
	\]
	
	$f \circ g$, si legge f composto g, ed è definito come:
	\[
	f \circ g := f(g(x)), \forall x\in X
	\]
	
	Es: avendo tre insiemi $\mathbb{Z}$, e due applicazioni $f$ e $g$, allora:
	\[
	\begin{array}{c c c c c}
		X & \xlongrightarrow{g} & Y & \xlongrightarrow{f} & Z \\
		n & \longmapsto & 3n+1 & & \\
		&& n & \longmapsto & n^2
	\end{array}
	\]
	
	Allora le composizioni di applicazioni sono:
	
	\[
	(f \circ g)(n) := f(3n + 1) = 9m^2 + 6m + 1
	\]	
	
	\[
	(g \circ f)(n) := g(n^2) = 3n^2 + 1
	\]
	Bisogna notare che in questo caso ha senso sia $f \circ g$ sia $g \circ f$, ma $(f \circ g) \not = (g \circ f)$
	
	
	OSS: 
	\[
	X \xlongrightarrow{g} Y \xlongrightarrow{f} X
	\]
	In questo caso e solo nel caso in cui l'insieme iniziale è lo stesso di quello finale, hanno senso sia $f \circ g$ sia $g \circ f$:
	\begin{center}
		Per $f \circ g : X \xlongrightarrow{f \circ g} X$ e per $g \circ f : Y \xlongrightarrow{g \circ f} Y$
	\end{center}
	Se $f \circ g = g \circ f$, allora $X = Y$, ma se $X = Y$ non è certo che $f \circ g = g \circ f$
	
	\section{Proprietà associativa della composizione}
	
	Avendo 4 insiemi $X, Y, W, Z$ e applicazioni $f, g, h$
	\[
	X \xlongrightarrow{f} Y \xlongrightarrow{g} W \xlongrightarrow{h} Z
	\]
	allora vale
	\[
	h \circ (g \circ f) = (h \circ g) \circ f
	\]
	Dimostrazione:
	\begin{align}
		(h \circ (g \circ f))(x) = h(g \circ f(x)) = h(g(f(x))) \\
		((h \circ g) \circ f))(x) = (h \circ g)(f(x)) = h(g(f(x)))
	\end{align}
	
	Perciò $\forall x \in X: (h \circ (g \circ f))(x) = ((h \circ g) \circ f))(x)$, quindi la composizione di applicazioni è una proprietà associativa, in cui il modo in cui si raggruppano le parentesi non cambia il risultato finale
	
	\section{Insieme identità di una applicazione}
	L'insieme identità di $X$ è l'applicazione:
	\[
	\begin{array}{c c c}
		X & \xlongrightarrow{Id_x} & X \\
		x & \longmapsto & x \\
	\end{array}
	\]
	Può essere espressa sia come $Id_x$ sia come $1_x$
	
	\setcounter{equation}{0}
	\begin{align}
		X & \xlongrightarrow{1_X} X \xlongrightarrow{f} Y \\
		X & \xlongrightarrow{f} Y \xlongrightarrow{1_Y} Y
	\end{align}
	Nel caso (1) l'applicazione composta $(f \circ 1_x) = f$ e nel caso (2) l'applicazione $(1_y \circ f) = f$.
	
	\section{Iniettività}
	Presi due insiemi $X, Y$ e una applicazione $f$:
	\[
	X \xlongrightarrow{f} Y
	\] 
	
	\begin{center}
		$f$ è iniettiva $\iff \forall x_1,x_2 \in X \rightarrow f(x_1) = f(x_2) \Rightarrow x_1 = x_2$.
	\end{center}
	
	Se presi due elementi $x_1, x_2 \in X$ allora gli elementi del codominio sono diversi se e solo se $x_1 \not = x_2$
	
	Es: 
	\[
	\begin{array}{c c c}
		\mathbb{R} & \xlongrightarrow{f} & \mathbb{R} \\
		x & \longmapsto & 3x + 1\\
	\end{array}
	\]
	è iniettiva in quanto:
	\[
	f(x_1) = f(x_2) \iff 3x_1 + 1 = 3x_2 + 1 \iff 3(x_1 - x_2) = 0 \iff x_1 = x_2
	\]
	
	\section{Suriettività}
	Presi due insiemi $X, Y$ e una applicazione $f$:
	\[
	X \xlongrightarrow{f} Y
	\] 
	
	\begin{center}
		$f$ è suriettiva $\iff \forall y \in Y, \exists x\in X \tc f(x) = y$.
	\end{center}
	
	L'immagine di $f$, ovvero, Im $f$ è definito come: 
	\[
	\text{Im} f:= \{y \in Y \tc \exists x\in X \tc f(x) = y\}
	\]
	OSS: f è suriettiva $\iff$ Im$f = Y$
	\section{Biettività}
	Presi due insiemi $X, Y$ e una applicazione $f$:
	\[
	X \xlongrightarrow{f} Y
	\] 
	
	\[f \text{ è biunivoca} \iff \forall y \in Y, \exists! x \in X \tc f(x) = y \]
	\[\text{(con } \exists! \text{ si intende esiste ed è unico})\]
	
	
	\section{Applicazione inversa}
	Presi due insiemi $X, Y$ e una applicazione $f$ biunivoca:
	\[
	X \xlongrightarrow{f} Y
	\] 
	
	L'applicazione inversa di $f$ è l'applicazione:
	\[
	\begin{array}{c c c}
		Y & \xlongrightarrow{f^{-1}} & X \\
		y & \longmapsto & x \in X \tc \exists ! x \tc f^{-1}(x) = y
	\end{array}
	\]
	
	Es:
	scelto
	\[
	\begin{array}{c c c}
		\mathbb{R} & \xlongrightarrow{f} & \mathbb{R} \\
		x & \longmapsto & 3x + 1
	\end{array}
	\]
	è biunivoca in quanto è sia suriettiva sia iniettiva, perciò è possibile trovare $f^{-1}$ risolvendo per x:
	\[
	3x + 1 = y \Longrightarrow 3x = y - 1 \Longrightarrow x = \dfrac{y -1}{3}
	\]
	Quindi l'applicazione inversa è:
	\[
	\begin{array}{c c c}
		\mathbb{R} & \xlongrightarrow{f^{-1}} & \mathbb{R} \\
		y & \longmapsto & \dfrac{y -1}{3}
	\end{array}
	\]
	
	È quindi possibile vedere che l'applicazione composta tra $f$ e $f^{-1}$ in qualsiasi ordine rappresenta l'applicazione identità:	
	\[
	\begin{array}{c c c c c c}
		X & \xlongrightarrow{f} & Y & \xlongrightarrow{f^{-1}} & X: & f^{-1} \circ f = Id_x \\
		Y & \xlongrightarrow{f^{-1}} & X & \xlongrightarrow{f} & Y: & f \circ f^{-1} = Id_x
	\end{array}
	\]\newline
	Presi due insiemi $X, Y$ e l'applicazione $f$:
	\[
	X \xlongrightarrow{f} Y
 	\]
 	
	se $y \in Y \rightarrow f^{-1}(y) := \{x \in X \tc f(x) = y\}$, in questo caso ha senso anche se non si tratta di applicazioni biunivoche in quanto restituisce un insieme e non un singolo elemento, quindi nel caso esistano più $x \tc f(x) = y$ si otterrà come risultato l'insieme numerico che contiene tutte le $x$
	
	\chapter{Relazioni}
	Preso $X$ un insieme, una relazione $\rel$ su X, è un sottoinsieme definito come: \[\rel \subset X^2 = X \times X\]
	Per $x \rel y$ si intende che $(x, y)\in \rel$
	
	Es 1: 
	\[X = \{\text{cittadini italiani}\}\]
	\[\rel = \{(x, y) \tc \text{x e y sono coetanei}\}\]
	\[x\rel y \rightarrow (x,y) \in \rel \rightarrow \text{x e y sono coetanei}\]
	Es 2:
	
	$X = \mathbb{Z} \text{, scelto } n \in \mathbb{Z}$
	\[
	\rel_n = \{(x, y) \tc n|(x - y), x,y \in X\} = x \equiv y \Mod{n}
	\]
	questa è la \textbf{relazione di congruenza modulo n}
	
	\section{Relazioni di equivalenza}
	Per relazione di equivalenza si intende una relazione $\rel$ su $X$, con $X$ un insieme qualsiasi, in cui valgono 3 proprietà:
	\begin{itemize}
		\item Riflessiva: $x \rel x, \text{ con } x \in X$
		\item Simmetrica: $x \rel y \rightarrow y \rel x, \text{ con } x,y \in X$
		\item Transitiva: $x \rel y \wedge y \rel z \rightarrow x \rel z, \text{ con } x,y,z \in X$
	\end{itemize}
	Controllo se l´esempio 2 è una relazione di equivalenza:
	\[
	X = \mathbb{Z}, n\in \mathbb{Z}
	\]
	\[
	x \rel y \text{ se } x \equiv y \Mod n
	\]
	\begin{itemize}
		\item Riflessiva: $x \equiv x \Mod n$ se $n|(x - x) \rightarrow n|0$, vera
		\item Simmetrica: $x \equiv y \Mod n$ se $n|(x - y) \rightarrow x-y = n*q \rightarrow -x + y = n * (-q)$, vera
		\item Transitiva: $x \equiv y \Mod n \wedge y \equiv z \Mod n \rightarrow x \equiv z \Mod{n}$, dimostrazione:
		\[
		n|(x-y) \wedge n|(y - x) = (x - y = q * n) \wedge (y - z = r * n)
		\]
		sommando due numeri multipli di n ottengo un multiplo di n
		\[
		(x -y) + (y -z) = q * n + r * n = n (q+r)
		\]
		quindi è verificata la proprietà transitiva
	\end{itemize}
	
	\subsection{Classe di equivalenza}
	Sia $x \in X$, allora la classe di equivalenza di x è:
	\[[x] := \{y \in X \tc x \rel y\}\]
	fissato x, $[x]$ è l'insieme composto dagli elementi $y \tc x\rel y$
	
	\subsection{Quoziente di X modulo R o insieme quoziente}
	Definiti $X$ un insieme e $\rel$ una relazione, il quoziente di X modulo $\rel$ è l'insieme i cui elementi sono le classi R-equivalenza, ovvero le classi di equivalenza rispetto alla relazione $\rel$ su $X$, quindi l'insieme quoziente è l'insieme composto dalle classi di equivalenza in $X$ generate da $\rel$ ed è rappresentato da: $X/R$
	
	\[
	X/R := \{[x] \tc x \in X\}
	\]
	
	In riferimento all'esempio 2:
	\[
	X/R_n := \{[0], [1], [2], ... , [n-1]\} %% check notazione
	\]e ogni elemento ad esempio [1], ha la proprio classe di equivalenza:
	\[
	[1] = \{y \in X \tc 1\rel y, 1 \equiv y \Mod{n}\}
	\]
	
	\section{Insieme delle parti}
	Una partizione $P$ di S è un insieme di sottoinsiemi $S_p \subset S \tc$
	\begin{itemize}
		\item $S_p \not = \emptyset, p \in P$
		\item $S_p \not = S_q \Rightarrow S_p \cap S_q = \emptyset$, quindi sono disgiunti.
		\item $\displaystyle \bigcup_{p \in P} S_p = S$, quindi l'unione di tutti i sotto insiemi coprono S.
	\end{itemize}
	
	Esempio:
	\[S = \{1,2,3\}\]
	
	allora, l'insieme delle parti di S è:
	\[P(S) := \{\{1,2,3\},\{\{1,2\}, \{3\}\},\{1,\{2,3\}\}, \{\{1,3\},\{2\}\},\{\{1\},\{2\},\{3\}\}\}\]
	Partendo da una relazione di equivalenza $\rel$ su X, $S := X/\rel$, con $x \in X$:
	\[[x] := \{y \in X \tc x \rel y\}\]
	
	allora, una partizione di S è definita come:
	\[P(S) := \{[x] \tc x \in X\}\]
	
	quindi una partizione di S, l'insieme quoziente $X/\rel$è l'insieme che contiene tutte le classi di equivalenza disgiunte. \newline
	Dimostrazione:
	\begin{enumerate}
		\item $[x] \not = \emptyset, \forall x \in \mathbb{R}, x \rel x \Rightarrow [x]$
		\item presi $[x], [y]$, si suppone che $z \in [x] \cap [y]$, se:
		\subitem $z \in [x] \rightarrow x \rel z$
		\subitem $z \in [y] \rightarrow y \rel z$ \newline
		allora $x \rel z \wedge y \rel z \iff x \rel y \Rightarrow [x] = [y]$
		\item $\displaystyle \bigcup_{x \in S} [x] = S \rightarrow \forall x \in S \Rightarrow x \in [x] \Rightarrow x \in \bigcup_{y \in S} [y]$
	\end{enumerate}
	Per ogni $\rel \longrightarrow P_{\rel_{\{[x]\}}}$ \newline %% TO ASK
	Per ogni partizione $P \longrightarrow \rel_P$
	\[P, Sp \subset S, x \rel y \iff x,y \in S \text{ per lo stesso p}\]
	
	\subsection{Lemma corrispondenza biunivoca tra relazione e partizione}
	Esiste una corrispondenza biunivoca tra:
	\[\{\rel \text{ su } S\} \iff \{\text{ Partizione di } S\}\]
	\[\rel \rightarrow P_{\rel} = \{[x] \tc x \in S\}\]
	\[(x \rel_P y \iff x,y \in S_p, p \in S )\leftarrow \rel_P \leftarrow P\]
	
	\chapter{Anello e campo}
	A è un insieme i cui elementi sono coppie ordinate $(x,y)$, ed ha 2 operazioni:
	\begin{itemize}
		\item somma: 
		\[
		\begin{array}{c c c}
			A \times A & \xlongrightarrow{somma} & A \\
			(x, y) & \longmapsto & x + y \\
		\end{array}
		\]
		\item prodotto:
		\[
		\begin{array}{c c c}
			A \times A & \xlongrightarrow{prodotto} & A \\
			(x, y) & \longmapsto & x * y \\
		\end{array}
		\]		
	\end{itemize}
	Le proprietà che rendono un insieme un anello o un campo sono:
	\begin{enumerate}
		\item Esistenza ed unicità dell'elemento neutro della somma, quindi:
		\[\exists! 0\in A \tc 0 + z = z + 0 = z \forall z\in A\]
		dimostrazione: presi $a \in A, b,c \in A \tc a + b = 0 \wedge a + c = 0, b \not = c$
		
		ipotesi: $b \stackrel{?}{=} c$
		\[0 = 0 \rightarrow a + b = a + c \rightarrow b = c\]
		\[Q.E.D\]
		\item Proprietà commutativa della somma, quindi: 
		\[z_1 + z_2 = z_2 + z_1, \forall z_1,z_2\in A\]
		\item Proprietà associativa della somma, quindi:
		\[z_1 + (z_2+z_3) = (z_1 + z_2) + z_3, \forall z_1,z_2,z_3\in A\]
		\item Esistenza ed unicità dell'opposto di $z \in A$, quindi:
		\[\forall z \in A \exists! w\in A \tc z + w = 0\]
		\item Proprietà associativa del prodotto, quindi:
		\[(z_1 * z_2) * z_3 = z_1 * (z_2 * z_3), \forall z_1,z_2,z_3\in A\]
		\item Proprietà distributiva del prodotto rispetto alla somma, quindi:
		\[z_1 * (z_2 + z_3) = z_1 * z_2 + z_1 * z_3 \wedge (z_1 + z_2) * z_3 = z_1 * z+3 + z_2 * z_3, \forall z_1,z_2,z_3\in A\]
		\item Proprietà commutativa rispetto al prodotto, quindi:
		\[w * z = z * w, \forall w,z \in A\]
		\item L'esistenza ed unicità dell'unità neutra del prodotto ($u \in A$) (unità moltiplicativa), diversa dall'unità neutra della somma, quindi $\not = 0$:
		\[u * x = x * u = x, \forall x \in A\]
		\item L'esistenza ed unicità dell'inverso moltiplicativo, quindi:
		\[\forall x\not = 0 \in A, \exists! w\in A \tc x * w = 1 \rightarrow w = x^{-1}\]
	\end{enumerate}
	
	\section{Anello}
	A (con le operazioni definite) è un anello se valgono le prime 6 proprietà (o assiomi).
	
	D'ora in avanti per completezza e facilità in diverse situazioni, per anello si intende un anello commutativo rispetto al prodotto e che presenta l'unità moltiplicativa, quindi in cui valgono anche le proprietà 7 e 8.
	
	\section{Campo}
	A (con le operazioni definite) è un campo se valgono le prime 9 proprietà (o assiomi).
	
	\section{Definizione di $\rel_n$ su $\mathbb{Z}$}
	\[x \rel_n y \iff n | (y - x) \iff y = x + kn, k\in \mathbb{Z}\]
	
	\subsection{Lemma}
	\[\mathbb{Z}/n\mathbb{Z} \text{ è un anello dato che:}\]
	\[\left\{\begin{aligned}
		[x] &+ [y] = [x + y] \\
		[x] &\cdot [y] = [x \cdot y]
	\end{aligned}\right\} \text{ sono ben definite}\]
	
	Dimostrazione:
	\[[x] = [x^\prime] \leftarrow x^\prime = x + nk\]
	\[[y] = [y^\prime] \leftarrow y^\prime = y + nk\]
	Verifichiamo:
	\[[x + y] = [x^\prime + y^\prime]\]
	\[x^\prime + y^\prime = x +nk + y + nl = x + y +n(k + l) \Rightarrow [x^\prime + y^\prime] = [x + y]\]
	\[[xy] = [x^\prime y^\prime]\]
	\[[x^\prime y^\prime] = (x +nk)(y + nl) = xy + n(xl + yk + nkl) \Rightarrow [x^\prime y^\prime] = [xy]\]
	Esempio di $\mathbb{Z}/n\mathbb{Z}$:
	
	\begin{table}[h!]
		\centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\caption*{Somma modulo 5}
			\[
			\begin{array}{c|ccccc}
				+_5 & 0 & 1 & 2 & 3 & 4 \\
				\hline
				0 & 0 & 1 & 2 & 3 & 4 \\
				1 & 1 & 2 & 3 & 4 & 0 \\
				2 & 2 & 3 & 4 & 0 & 1 \\
				3 & 3 & 4 & 0 & 1 & 2 \\
				4 & 4 & 0 & 1 & 2 & 3 \\
			\end{array}
			\]
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\centering
			\caption*{Prodotto modulo 5}
			\[
			\begin{array}{c|ccccc}
				\cdot_5 & 0 & 1 & 2 & 3 & 4 \\
				\hline
				0 & 0 & 0 & 0 & 0 & 0 \\
				1 & 0 & 1 & 2 & 3 & 4 \\
				2 & 0 & 2 & 4 & 1 & 3 \\
				3 & 0 & 3 & 1 & 4 & 2 \\
				4 & 0 & 4 & 3 & 2 & 1 \\
			\end{array}
			\]
		\end{minipage}
	\end{table}
	
	\begin{table}[h!]
		\centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\caption*{Somma modulo 6}
			\[
			\begin{array}{c|cccccc}
				+_6 & 0 & 1 & 2 & 3 & 4 & 5 \\
				\hline
				0 & 0 & 1 & 2 & 3 & 4 & 5 \\
				1 & 1 & 2 & 3 & 4 & 5 & 0 \\
				2 & 2 & 3 & 4 & 5 & 0 & 1 \\
				3 & 3 & 4 & 5 & 0 & 1 & 2 \\
				4 & 4 & 5 & 0 & 1 & 2 & 3 \\
				5 & 5 & 0 & 1 & 2 & 3 & 4 \\
			\end{array}
			\]
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\centering
			\caption*{Prodotto modulo 6}
			\[
			\begin{array}{c|cccccc}
				\cdot_6 & 0 & 1 & 2 & 3 & 4 & 5 \\
				\hline
				0 & 0 & 0 & 0 & 0 & 0 & 0 \\
				1 & 0 & 1 & 2 & 3 & 4 & 5 \\
				2 & 0 & 2 & 4 & 0 & 2 & 4 \\
				3 & 0 & 3 & 0 & 3 & 0 & 3 \\
				4 & 0 & 4 & 2 & 0 & 4 & 2 \\
				5 & 0 & 5 & 4 & 3 & 2 & 1 \\
			\end{array}
			\]
		\end{minipage}
	\end{table}	
	Mettendo a confronto le tabelle 2 e 4, si può notare che per la tabella 4 non esiste l'inverso moltiplicativo.
	
	\subsection{Teorema del campo, con n primo}
	$\mathbb{Z}/n\mathbb{Z}$ è un campo $\iff$ n è primo.
	Dimostrazione per assurdo:
	\[n = m*l, 0 < m < n \wedge 0<l<n\]
	\[m*l = 0 \Mod{n}\]
	se $\exists \frac{1}{m} \rightarrow \frac{1}{m} * m *l = 0 \Rightarrow l = 0$, però si ha una contraddizione in quanto  l è maggiore di 0, quindi n deve essere primo.
	
	\subsection{Lemma di Bezout}
	\teorem{
		Se si hanno due numeri \( a, b \) con \( \mathrm{MCD}(a,b) = d \), allora \( \exists x, y \in \mathbb{Z} \tc ax + by = d \).
	}
	
	Quindi applicando il teorema di Bezout ad a e p, in modo che $MCD(a, p) = 1$, allora:
	\[\exists x,y \in \mathbb{Z} \tc ax + py = 1 \longrightarrow \mathbb{Z}/p\mathbb{Z}, ax = 1 \Mod p \rightarrow x = \frac{1}{a}\]
	Il termine $py$ scompare in quanto applicando il modulo p all'insieme, rendendolo un insieme quoziente, $py$, è uguale a 0.
	\chapter{Numeri complessi}
	Definite due operazioni in $\mathbb{R}^2$:
	\begin{itemize}
		\item Somma:
		\[(x_1, y_1), (x_2, y_2) \in \mathbb{R}^2\]
		\[(x_1, y_1) + (x_2, y_2) := (x_1 + x_2, y_1 + y_2)\]
		\item Prodotto:
		\[(x_1, y_1), (x_2, y_2) \in \mathbb{R}^2\]
		\[(x_1, y_1) * (x_2, y_2) := ((x_1 * x_2 - y_1*y_2), (x_1 * y_2 + x_2 * y_1))\]
	\end{itemize}
	il prodotto è così perchè se fosse definito come$(x_1 * x_2 , y_1 * y_2)$ allora $\mathbb{R}^2$ non sarebbe un anello (commutativo ed unitario)
	
	\section{Verifica $\mathbb{R}^2$}
	Verifichiamo che $\mathbb{R}^2$ con le proprietà definite è un campo:
	\begin{enumerate}
		\item Unicità dell'elemento neutro della somma:
		\[(0, 0) \not = (0^\prime, 0^\prime)\]
		\[(0, 0) + (x, y) = (x,y) \wedge (0^\prime, 0^\prime) + (x, y) = (x, y)\]
		\[(x, y) = (x, y) \rightarrow (0, 0) = (0^\prime, 0^\prime)\]
		\item Proprietà commutativa della somma:
		\[(a, b) + (c, d) = (a + c, b + d) = (c + d) + (a + b)\]
		la proprietà commutativa degli elementi $a,b,c,d$ non va dimostrata in questo momento essendo essi elementi di $\mathbb{R}$ commutativi.
		\item Associatività della somma:
		\[(a, b) + ((c, d) + (e, f)) = (a, b) + (c + e, d + f) = (c + e + a, d + f+ b)\]
		\[((a,b) +  (c, d)) + (e, f) = (a + c, b + d) + (e, f) = (a + c + e, b + d + f)\]
		\[(c + e + a, d + f+ b) = (a + c + e, b + d + f)\]
		per la proprietà commutativa in $\mathbb{R}$
		\item Esistenza ed unicità dell'opposto:
		\[(z, w) \not = (z^\prime, w^\prime)\]
		\[(x, y) + (z, w) = (0, 0) \wedge (x, y) + (z^\prime, w^\prime) = (0, 0)\]
		\[(0, 0) = (0, 0) \rightarrow (x, y) + (z, w) = (x, y) + (z^\prime, w^\prime)\]
		\[(z, w) = (z^\prime, w^\prime)\]
		quindi esiste ed è unico l'opposto della somma
	\end{enumerate}
	
	Oss: 
	\[(1, 0) * (x, y) = (1 * x - 0 * y, 1 * y + 0 * x) = (x, y)\]
	quindi la coppia $(1, 0)$ è l'elemento neutro del prodotto
	
	\section{Notazione}
	\[
	\left\{
	\begin{aligned}
		1:= (1, 0) \quad & i := (0, 1) \\
		\forall a \in \mathbb{R} \quad \quad & a(x, y) :=(ax, ay) \\
		i^2 = -1 \quad \;\;\;& 
	\end{aligned}
	\right.
	\]
	Un numero complesso $z = a1 + bi = a + bi := (a, 0) + (0, b) = (a, b)$
	
	Allora abbiamo che un prodotto tra numeri complessi è in questa forma:
	\[(a + bi) * (c + di) = a * c - bi * c + a * di + b * d * i^2 = (a*c - b*d + (a*d + b*c)i)\]
	Il prodotto in C, tra coppie, deriva:
	\[z = a + bi = (a, b) \wedge w = c + di = (c, d)\] 
	\[z * w = (a, b) * (c, d) = (a + bi) * (c + di)\]
	A questo punto svolgo le normali moltiplicazioni tra parentesi in R, perché non sono più coppie, ma sono numeri,  da questo ottengo una somma tra una parte reale e una parte immaginaria, da cui deriva il prodotto in $\mathbb{C}$.
	\[(a*c - b*d + (a*d + b*c)i) = ((a*c - b*d), (a*d + b*c))\]
	
	\section{Continuo verifica $\mathbb{R}$}
	continuo con la verifica delle proprietà dalla 5 alla 9
	\begin{enumerate}
		\setcounter{enumi}{4} % parte da 3 perché LaTeX aggiunge +1
		\item Associatività del prodotto:
		\[((a, b) * (c, d)) * (x, y) = (a * c - b * d, a*d + b*c) * (x, y)\]
		\[(A, B) := (a * c - b * d, a*d + b*c) \rightarrow (A, B) * (x, y)\]
		\[(A * x - B*y, A*y + B*x) = ((a * c - b * d) * x - (a*d + b*c)*y, (a * c - b * d) * y + (a*d + b*c)*x)\]
		\[((a * c * x - b*d*x - a*d*y -b*c*y), (a*c*y - b*d*y + a*d*x+b*c*x))\]
		ora vediamo se la proprietà associativa vale:
		\[(a,b) * ((c, d) * (x, y)) = (a,b) * (c*x - d*y, c*y + d*x)\]		
		\[(C, D) := (c*x - d*y, c*y + d*x) \rightarrow (a,b) * (C, D)\]
		\[(a* C - b*D, a*D + b*C) = (a*(c*x - d*y) - b*(c*y + d*x), a*(c*y + d*x) + b*(c*x - d*y))\]
		\[((a*c*x - a*d*y - b*c*y - b*d*x), (a*c*y + a*d*x + b*c*x - b*d*y))\]
		
		secondo la proprietà commutativa della somma, i due prodotti sono equivalenti, quindi la proprietà associativa è verificata
			
		\item Proprietà associativa del prodotto rispetto alla somma:
		\[z_1 * (z_2 + z_3) = (a,b) * ((c,d) + (x,y))\]		
		\[(c,d) + (x,y) = (c+x, d+y) \rightarrow (a,b) * (c+x, d+y)\]		
		\[(a,b)*(c+x, d+y) = (a*(c+x) - b*(d+y), a*(d+y) + b*(c+x))\]		
		\[(a*c + a*x - b*d - b*y, a*d + a*y + b*c + b*x)\]		
		\[(a*c - b*d, a*d + b*c) + (a*x - b*y, a*y + b*x)\]			
		\[(a,b)*(c,d) + (a,b)*(x,y)\]		
		\[\Rightarrow z_1*(z_2 + z_3) = z_1*z_2 + z_1*z_3\]
		
		\item unità neutra del prodotto
				
		\item Esistenza ed unicità dell'inverso moltiplicativo:
		\[(a, b) * (x, y) = (1, 0) = 1 + 0i\]
		\[(ax - by, ay + bx)\]
		Dato che vogliamo che la parte reale del prodotto sia uguale a uno e quella immaginaria sia zero, possiamo scrivere questa equazione come un sistema.
		\[
		\left\{
		\begin{aligned}
			ax - by & = 1\\
			ay + bx & = 0
		\end{aligned}
		\right.
		\]
		Dal secondo:
		\[ay + bx = 0 \implies y = -\frac{b}{a}x, a \not = 0\]
		Sostituendo nella prima equazione:
		\[ax - b(-\frac{b}{a}x) = 1 \implies ax + \frac{b^2}{a}x = 1 \implies a^2x+b^2x = a \implies x = \frac{a}{a^2 + b^2}\]
		Sapendo che $y = -\frac{b}{a}x$, allora:
		\[y = -\frac{b}{a} \frac{a}{a^2 + b^2} = \frac{b}{a^2 + b^2}\]
		Quindi abbiamo dimostrato che la coppia $\left(\dfrac{a}{a^2 + b^2}, \dfrac{b}{a^2 + b^2}\right) = (a, b)^{-1}$ quindi:
		\[(a, b) * \left(\dfrac{a}{a^2 + b^2}, \dfrac{b}{a^2 + b^2}\right) = (1, 0)\] 
	\end{enumerate}
	
	\section{Caratteristiche}
	Un numero $z \in \mathbb{C} = a + bi, a,b \in \mathbb{R}$, e con $\mathbb{R}$ in intende $\mathbb{R}^2$ con le operazioni di somma e prodotto definite.
	La parte reale dei numeri complessi è definita da $Re(z) := a$ e la parte immaginaria $Im(z) := b$
	
	\section{Rappresentazione grafica}
	Scelto un sistema di coordinate cartesiano del piano, allora ad $a + bi \in C$, corrisponde un punto/vettore $P(a,b)$. Presi $z = a+bi, w = c+ di$
	\begin{center}
		\begin{tikzpicture}[->, thick]
			% assi
			\draw[->] (-3,0) -- (3,0) node[above] {$Re$};
			\draw[->] (0,-3) -- (0,3) node[above] {$Im$};
			
			\fill (1, 0) circle (2pt) node[below] {$(1, 0)$};
			\fill (0, 1) circle (2pt) node[left] {$(0, 1)$};
			
			% vettore
			\draw[->, red] (0,0) -- (0.5,1) node[above] {$z$};
			\draw[->, blue] (0,0) -- (2,0.5) node[above] {$w$};
			
			
			\draw[->, red, dashed] (2,0.5) -- (2.5, 1.5) node[above] {};
			
			\draw[->, darkgray] (0, 0) -- (2.5, 1.5) node[above] {$w + z$};
		\end{tikzpicture}
	\end{center}
	la somma tra due numeri $w,z \in \mathbb{C}$ equivale alla somma tra vettori, quindi esegui una normale somma tra le componenti:
	\[z = a+bi = (a,b), w = c+di = (c,d) \Rightarrow z + w = (a,b)+(c,d) = (a+c, b+d)\]
	Per il prodotto tra $z_1, z_2 \in \mathbb{C}$ bisogna definire il modulo di numero complesso:
	\[|z| = \rho := \sqrt{a^2 + b^2}\]
	e l'argomento di un numero complesso:
	\[Arg(z) = \theta, \text{ determinato a meno di un multiplo intero di 2$\pi$}, z\not = 0\]
	
	\begin{center}
		\begin{tikzpicture}[->, thick]
			    % assi
			\draw[->] (-3,0) -- (3,0) node[above] {$Re$};
			\draw[->] (0,-3) -- (0,3) node[above] {$Im$};
			
			% punti base
			\fill (1,0) circle (2pt) node[below] {$(1,0)$};
			\fill (0,1) circle (2pt) node[left] {$(0,1)$};
			
			% vettore z
			\draw[->, black, thick] (0,0) -- (2,0.8) node[above] {$z$};
			
			\draw[black] (0,0) -- (2,0.8) node[midway, above] {$\rho$};
			
			% nodi
			\coordinate (X) at (2,0);
			\coordinate (O) at (0,0);
			\coordinate (Z) at (2,0.8);
			
			% angolo senza freccia
			\pic [draw=black, "$\theta$", angle eccentricity=1.5, angle radius=1cm, mark=none]
			{angle = X--O--Z};
		\end{tikzpicture}
	\end{center}
	
	sapendo che le coordinate di z son $(a, b)$, allora posso definirle in funzione di $\rho, \theta$:
	\[
	\left\{
	\begin{aligned}
		a =& \rho * \cos\theta\\
		b =& \rho * \sin\theta
	\end{aligned}
	\right. \Rightarrow z = \rho * \cos\theta + i\rho * \sin\theta = \rho( \cos\theta + i\sin\theta)
	\]
	Allora presi: $z_1 = \rho_1( \cos\theta_1 + i \sin\theta_1), z_2 = \rho_2( \cos\theta_2 + i\sin\theta_2)$
	\[ z * w = \rho_1 * \rho_2(\cos\theta_1 + i\sin\theta_1) * (\cos\theta_2 + i\sin\theta_2) = \]
	\[ = \rho_1 * \rho_2(\cos\theta_1 * \cos\theta_2 - \sin\theta_1 * \sin\theta_2) + i(\cos\theta_1 * \sin\theta_2 + \sin\theta_1 * \cos\theta_2 ) = \]
	\[= \rho_1 * \rho_2(\cos(\theta_1+\theta_2) + i\sin(\theta_1+\theta_2))\]
	quindi:
	\[
	\left\{
	\begin{aligned}
		|z_1 * z_2| = |z_1| * |z_1| = \rho_1 * \rho_2 \quad \quad \\
		Arg(z_1 * z_2) = Arg(z_1) + Arg(z_2)
	\end{aligned}\right.\]
	$Arg(z_1 * z_2)$ è determinato a meno di un multiplo intero di 2$\pi$
	
	\begin{center}
		\begin{tikzpicture}[->, thick]
			% assi
			\draw[->] (-3,0) -- (3,0) node[above] {$Re$};
			\draw[->] (0,-3) -- (0,3) node[above] {$Im$};
			
			% punti base
			\fill (1,0) circle (2pt) node[below] {$(1,0)$};
			\fill (0,1) circle (2pt) node[left] {$(0,1)$};
			
			% vettore z_1
			\draw[->, black, thick] (0,0) -- (2,0.8) node[above] {$z_1$};
			
			\draw[black] (0,0) -- (2,0.8) node[midway, above] {$\rho_1$};
			
			% nodi
			\coordinate (X) at (2,0);
			\coordinate (O) at (0,0);
			\coordinate (Z) at (2,0.8);
			
			% angolo senza freccia
			\pic [draw=black, "$\theta_1$", angle eccentricity=1.5, angle radius=1cm, mark=none]
			{angle = X--O--Z};
			
			% vettore z_2
			\draw[->, black, thick] (0,0) -- (2,2.2) node[above] {$z_2$};
			
			\draw[black] (0,0) -- (2,2.2) node[midway, above] {$\rho_2$};
			
			% nodi
			\coordinate (Y) at (2,0);
			\coordinate (O) at (0,0);
			\coordinate (K) at (2,2.2);
			
			% angolo senza freccia
			\pic [draw=black, "$\theta_2$", angle eccentricity=1.3, angle radius=1.5cm, mark=none]			
			{angle = Y--O--K};
			
			% vettore prodotto
    		\draw[->, red, thick] (0,0) -- (69.5:6.1) node[above] {$z_1 * z_2$};
			
		\end{tikzpicture}
	\end{center}	
	
	moltiplicare un numero complesso per $i$, vuol dire ruotarlo di 90 gradi verso sinistra
	
	\section{Teorema fondamentale dell'algebra}
	Sia $P(z) = a_0z^n + a_1z^{n-1} + a_2z^{n-2} + ... + a_n$, polinomio in $z$ a coefficienti in $\mathbb{C}$ di grado $n > 0$ con $a_0 \not = 0$, allora:
	\[\exists \xi \in \mathbb{C} \tc P(\xi) = 0\]
	($\xi$ è una "radice di $P(z) = 0$")
	
	T.F.A $\implies \exists \lambda_1, \lambda_2, ..., \lambda_n \in \mathbb{C} \tc P(z) = a_0(z - \lambda_1)(z - \lambda_2) * ... * (z - \lambda_n)$
	
	Per via del teorema di Ruffini in quanto:
	\begin{center}
		essendo $\lambda_1$ radice di $P(z) = 0 \rightarrow P(z) = (z - \lambda_1)*q(z)$, in cui il grado di $q(z)$ = grado di $P(z)$ - 1
	\end{center}
	
	Es:
	
	\[p(z) = z^n - a\]
	si cercano le radici di $z^n - a = 0$:
	\begin{itemize}
		\item $a = 0 \rightarrow z^n = 0$, la radice è unica con molteplicità $n$
		\item $a \not = 0$, se $n = 3$:
		\[
		\begin{aligned}
			z &= \rho(\cos\theta + i\sin\theta) \\
			z^n &= \rho(\cos n\theta + i\sin n\theta) \\
			a \, \, &= \rho_0(\cos\theta_0 + i\sin\theta_0)
		\end{aligned}
		\]
		\[z^n = a \iff \left\{
		\begin{aligned}
			\rho^n &= \rho_0 \rightarrow \rho = \sqrt[n]{\rho_0}\\
			n\theta &= \theta_0 \text{ a meno di multipli interi di } 2\pi
		\end{aligned} \right.\]
		
		\[Arg(z = \sqrt[n]{a}) = \theta = \left\{\frac{\theta_0}{n} + \frac{k2\pi}{n}, k \in \{0, 1, ..., n -1\}\right\}\]
		
	\end{itemize}
	
	\chapter{Spazi vettoriali}
	$\mathbb{K}$ è un campo, $\mathbb{K}^n = \mathbb{K} \times \dots \times \mathbb{K} = \{(x_1, \dots , x_n) \tc x_i \in \mathbb{K}\}$, usato una notazione impropria possiamo definire la n-tupla $(x_1, \dots , x_n)$, come $X$
	
	Definiamo l'operazione di somma tra n-tuple:
	\[
	\begin{array}{c c c}
		\mathbb{K}^n \times \mathbb{K}^n & \xlongrightarrow{somma} & \mathbb{K}^n \\
		(X, Y) & \longmapsto & (x_1 + y_1, \dots, x_n + y_n) \\
	\end{array}
	\]	
	ed il prodotto per uno scalare:
	\[
	\begin{array}{c c c}
		\mathbb{K} \times \mathbb{K}^n & \xlongrightarrow{prodotto} & \mathbb{K}^n \\
		(\lambda, X) & \longmapsto & (\lambda x_1, \dots, \lambda x_n) \\
	\end{array}
	\]	

	$\mathbb{K}^n$ con le proprietà definite è il prototipo di uno spazio vettoriale su $\mathbb{K}$.
	
	Uno spazio vettoriale su $\mathbb{K}$ è definito come un insieme $\mathbb{V}$, provvisto di una operazione somma:
	\[
	\begin{array}{c c c}
		\mathbb{V} \times \mathbb{V} & \xlongrightarrow{somma} & \mathbb{V} \\
		(v, w) & \longmapsto & v + w \\
	\end{array}
	\]	
	e il prodotto per uno scalare:
	\[
	\begin{array}{c c c}
		\mathbb{K} \times \mathbb{V} & \xlongrightarrow{prodotto} & \mathbb{V} \\
		(\lambda, v) & \longmapsto & \lambda v \\
	\end{array}
	\]	
	
	tali che:
	\begin{enumerate}
		\item $\exists \underline{0} \in \mathbb{V} \tc \underline{0} + v = v, \forall v \in \mathbb{V}$
		\item Dato $v \in \mathbb{V}, \exists w \in \mathbb{V} \tc v + w = \underline{0}$
		\item $(v + w) + u = v + (w + u) \forall v,w,u \in \mathbb{V}$
		\item $u + v = v + u, \forall u,v \in \mathbb{V}$ 
		\item $1v = v$
		\item $v(\lambda + \mu) = \lambda v + \mu v, \forall \lambda, \mu \in \mathbb{K}, \forall v \in \mathbb{V}$
		\item $\lambda (v + w) = \lambda v + \lambda w, \forall \lambda \in \mathbb{K}, \forall v, w \in \mathbb{V}$
		\item $(\lambda \mu)v = \lambda(\mu v), \forall \lambda, \mu \in \mathbb{K}, \forall u \in \mathbb{V}$
	\end{enumerate}
	Osservazione, l'elemento neutro di $\mathbb{K}$ è diverso dall'elemento neutro di $\mathbb{V}$:
	\[0 \in \mathbb{K} \not = \underline{0} \in \mathbb{V}\]
	\[0 \not = (0, \dots, 0)\]
	Proposizione: $\mathbb{V}$ sp. vett/$\mathbb{K}$:
	\begin{enumerate}
		\item Esiste un unico elemento neutro di $\mathbb{V}$, dimostrazione:
		\[\underline{0}_1 + v = v \quad \wedge \quad \underline{0}_2 + v = v, \forall v \in \mathbb{V}\]
		\[\underline{0}_2 = \underline{0}_1 + \underline{0_2} = \underline{0}_1 \Rightarrow \underline{0}_1 = \underline{0}_2 = \underline{0}\]
		\item Dato $v \in \mathbb{V}$ esiste un unico $w \in \mathbb{V} \tc v + w = \underline{0}$
		\item $0 \cdot v = \underline{0}$ 
	\end{enumerate}
	
	\section{Vettori nel piano euclideo}
	Sia $\mathbb{E}^2$, il piano euclideo, allora un vettore nel piano è una classe di equivalenza di segmenti orientati nel piano, indipendenti dalla loro posizione.
	Presi $A, (x_a, y_a) \not = B, (x_b, y_b) \in \mathbb{E}^2, \overline{AB}$ è l'unica retta in $\mathbb{E}^2$ contenente contemporaneamente A e B.
	
	\begin{center}
		\begin{tikzpicture}
		% Coordinate dei punti A e B
		\coordinate (A) at (1,1);
		\coordinate (B) at (4,2);
		
		% Vettore direzione = (B - A)
		\coordinate (v) at ($(B)-(A)$);
		
		% Fattore di estensione (riducilo per accorciare la retta)
		\def\k{0.5}
		
		% Inizio e fine della retta estesa
		\coordinate (P1) at ($(A) - \k*(v)$);
		\coordinate (P2) at ($(B) + \k*(v)$);
		
		% Disegno della retta estesa
		\draw[thick] (P1) -- (P2);
		
		% Punti A e B
		\filldraw (A) circle (2pt) node[below] {$A$};
		\filldraw (B) circle (2pt) node[above] {$B$};
		\end{tikzpicture}
	\end{center}
	
	\begin{itemize}
		\item Due rette $R_1, R_2$ in $\mathbb{E}^2$ sono parallele se:
		$\left\{\begin{aligned}
			R_1 & \cap R_2 = \emptyset \\
			R_1 & = R_2
		\end{aligned}\right.$
		
		\item Presi i punti: $A,B,C,D \in \mathbb{E}^2, \overline{AB} \parallel \overline{CD}$, significa che:
		\[
		\left\{\begin{aligned}
			A &\not = B, C \not = D, \text{ quindi le retta $\overline{AB}$ e $\overline{CD}$ sono parallele}\\
			A & = B \vee C = D
		\end{aligned}\right.
		\]
	\end{itemize}
	Un segmento orientato in $\mathbb{E}^2$ è una coppia ordinata $(A, B) = ((x_a, y_a), (x_b, y_b)) = \mathbb{E}^2 \times \mathbb{E}^2$, con $A, B \in \mathbb{E}^2$ e si definisce per semplicità come $AB := (A, B)$
	
	Due segmenti orientati $AB, CD$ si dicono equipollenti se:
	\[AB \parallel CB \text{ e } \overline{AC} \parallel \overline{BD}\]
	
	\begin{center}
		\begin{comment}
			\begin{tikzpicture}[scale=0.75]
		
		% Coordinate dei punti
		\coordinate (A) at (0,0);
		\coordinate (B) at (4,2);
		\coordinate (C) at (1,3);
		
		% Vettore AB per parallela
		\coordinate (v) at ($(B)-(A)$);
		
		% Estensione della retta parallela ad AB che passa per C
		\draw[blue, thick, dashed] ($(C) - 1.2*(v)$) -- ($(C) + 1.2*(v)$) node[right] {retta // \(AB\)};
		
		% Segmento AB
		\draw[thick, ->] (A) -- (B) node[midway, below right] {\(AB\)};
		
		% Punti
		\filldraw (A) circle (2pt) node[below left] {\(A\)};
		\filldraw (B) circle (2pt) node[below right] {\(B\)};
		\filldraw (C) circle (2pt) node[above left] {\(C\)};
		
	\end{tikzpicture}
		\end{comment}
	\begin{tikzpicture}[scale=0.75]
		
		% Coordinate dei punti
		\coordinate (A) at (0,0);
		\coordinate (B) at (4,2);
		\coordinate (C) at (1,3);
		
		% Vettori direzione
		\coordinate (vAB) at ($(B)-(A)$);
		\coordinate (vAC) at ($(C)-(A)$);
		
		% Retta per C parallela ad AB
		\path[name path=parallelAB] 
		($(C) - 1.5*(vAB)$) -- ($(C) + 1.5*(vAB)$);
		%\draw[blue, thick, dashed] 
		%($(C) - 1.1*(vAB)$) -- ($(C) + 1.5*(vAB)$)
		%node[below right] {retta // \(AB\)};
		
		% Retta per B parallela ad AC
		\path[name path=parallelAC]
		($(B) - 1.5*(vAC)$) -- ($(B) + 1.5*(vAC)$);
		\draw[red, thick, dashed]
		($(B) - 1.5*(vAC)$) -- ($(B) + 1.5*(vAC)$)
		node[above right] {\(\overline{BD}\) // \(\overline{AC}\)};
		
		% Retta AC vera e propria
		\draw[thick, dashed, gray]
		($(A) - 1*(vAC)$) -- ($(C) + 1.2*(vAC)$)
		node[below left] {\(\overline{AC}\)};
		
		% Intersezione delle due rette → punto D
		\path [name intersections={of=parallelAB and parallelAC, by=D}];
		
		% Disegno punto D
		\filldraw (D) circle (2pt) node[above right] {\(D\)};
		
		% Segmento AB
		\draw[thick, ->] (A) -- (B) node[midway, below right] {\(AB\)};
		\draw[thick, ->, blue] (C) -- (D) node[midway, below right] {\(CD\)};
		
		% Punti noti
		\filldraw (A) circle (2pt) node[below left] {\(A\)};
		\filldraw (B) circle (2pt) node[below right] {\(B\)};
		\filldraw (C) circle (2pt) node[above left] {\(C\)};
		
	\end{tikzpicture}
	\end{center}
	
	\begin{itemize}
		\item ogni segmento orientato è equipollente a se stesso
		\item se $S_1$ è equipollente a $S_2 \Rightarrow S_2$ è equipollente $S_1$
		\item se $S_1$ è equipollente a $S_2$ e $S_2$ è equipollente a $S_3$, allora $S_1$ è equipollente a $S_3$
	\end{itemize}
	Perciò l'equipollenza è una relazione di equivalenza, definita da: $\sim$.
	
	\[V(\mathbb{E}^2) := (\mathbb{E}^2 \times \mathbb{E}^2)/\sim\]
	\[\vec{AB} := [AB]\]
	
	quindi $V(\mathbb{E}^2)$ è l'insieme quoziente, definito dalla equipollenza, una relazione di equivalenza, perciò i suoi elementi sono tutte le classi di equivalenza contenenti tutti i possibili vettori con la stessa direzione e lunghezza, perciò l'insieme $V(\mathbb{E}^2)$ contiene tutti i possibili vettori nel piano euclideo, ed ogni classe di equivalenza si rappresenta come un rappresentate scelto con una freccia sopra, per definire la direzione.
	
	Definisco la somma in $V(\mathbb{E}^2)$:
	\[
	\begin{array}{c c c}
		\mathbb{V}(\mathbb{E}^2) \times \mathbb{V}(\mathbb{E}^2) & \xlongrightarrow{somma} & \mathbb{V}(\mathbb{E}^2) \\
		(\vec{AB}, \vec{CD}) & \longmapsto & \vec{AP} \\
	\end{array}
	\]	
	
	Presi due vettori $\vec{AB}, \vec{CD}, \exists P \in \mathbb{E}^2 \tc \vec{BP} \sim \vec{CD}$
	
	\begin{center}
		\begin{tikzpicture}[scale=1.2]
		
		% Definizione punti A, B, C, D
		\coordinate (A) at (0,2);
		\coordinate (B) at (2,1);
		\coordinate (C) at (1,3);
		\coordinate (D) at (4,4);
		
		% Calcolo vettore CD
		\coordinate (vCD) at ($(D)-(C)$);
		
		% Calcolo punto P tale che BP = CD
		\coordinate (P) at ($(B)+(vCD)$);
		
		% Vettore AB
		\draw[->, thick, blue] (A) -- (B) node[midway, below left] {\(\vec{AB}\)};
		
		% Vettore CD
		\draw[->, thick, red] (C) -- (D) node[midway, above] {\(\vec{CD}\)};
		
		% Vettore BP (equipollente a CD)
		\draw[->, thick, green!60!black] (B) -- (P) node[midway, below right] {\(\vec{BP}\)};
		
		\draw[->, thick, black] (A) -- (P) node[midway, below right] {\(\vec{AP}\)};
		
		% Disegno dei punti
		\filldraw (A) circle (2pt) node[below left] {\(A\)};
		\filldraw (B) circle (2pt) node[below right] {\(B\)};
		\filldraw (C) circle (2pt) node[above left] {\(C\)};
		\filldraw (D) circle (2pt) node[above right] {\(D\)};
		\filldraw (P) circle (2pt) node[right] {\(P\)};
		
	\end{tikzpicture}
	\end{center}
	
	Definisco il prodotto per uno scalare $\mathbb{R}$:
		\[
	\begin{array}{c c c}
		\mathbb{R} \times \mathbb{V}(\mathbb{E}^2) & \xlongrightarrow{prodotto} & \mathbb{V}(\mathbb{E}^2) \\
		(\lambda, \mathbb{V}(\mathbb{E}^2)) & \longmapsto & \vec{AP} \\
	\end{array}
	\]	
	
	\begin{center}
		\begin{tikzpicture}[scale=1]
			
			% Coordinate dei punti
			\coordinate (A) at (0,0);
			\coordinate (B) at (3,1);
			
			% Vettore AB per direzione
			\coordinate (v) at ($(B)-(A)$);
			
			% Coordinate dei nuovi punti: p dopo B, P prima di A
			\coordinate (p) at ($(B) + 0.8*(v)$);
			\coordinate (P) at ($(A) - 0.8*(v)$);
			
			% Retta parallela (passante per A, lungo v)
			\draw[blue, thick, dashed] 
			($(A) - 1.2*(v)$) -- ($(A) + 1.9*(v)$) 
			node[right] {\(\overline{AB}\)};
			
			% Segmento AB
			\draw[thick, ->] (A) -- (B) node[midway, below right] {\(AB\)};
			
			% Punti principali
			\filldraw (A) circle (2pt) node[below left] {\(A\)};
			\filldraw (B) circle (2pt) node[below right] {\(B\)};
			
			% Nuovi punti p e P
			\filldraw (p) circle (2pt) node[below right] {\(p\)};
			\filldraw (P) circle (2pt) node[above left] {\(P\)};
			
		\end{tikzpicture}
	\end{center}
	
	\[\vec{AP} = \lambda * \vec{AB}, \lambda < 0\]
	\[\vec{Ap} = \lambda * \vec{AB}, \lambda > 0\]
	
	Con queste operazioni $V(\mathbb{E}^2)$ è uno spazio vettoriale/$\mathbb{R}$
	
	\section{Sottospazi vettoriali}
	Preso $\mathbb{V}$ un sp.vett./$\mathbb{K}$, allora $\mathbb{W} \subset \mathbb{V}$ è un sottospazio vettoriale se:
	\begin{itemize}
		\item non è vuoto, quindi:
		\[\mathbb{W} \not = \emptyset\]
		\item a) è chiuso per la somma:
		\[v_1, v_2 \in \mathbb{W} \Rightarrow v_1 + v_2 \in \mathbb{W}\]
		\item b) è chiuso per il prodotto per scalare:
		\[w \in \mathbb{W}, \lambda \in \mathbb{K} \Rightarrow \lambda w \in \mathbb{W}\]
	\end{itemize}
	Osservazione: Sia $\mathbb{W} \subset \mathbb{V}$ è un sottospazio:
	\begin{itemize}
		\item $\underline{0} \in \mathbb{W}$
		\item $w \in \mathbb{W} \xRightarrow {b} (-1)w \in \mathbb{W}$
		\item $w \in \mathbb{W}, (-1)w \in \mathbb{W} \Rightarrow 1w + (-1)w = w(1 - 1) = 0w \xlongrightarrow{a} \underline{0} \in \mathbb{W}$
	\end{itemize}
	Sia $\mathbb{W} \subset \mathbb{K}^n$, un sottoinsieme, dato da:
	\[\mathbb{W} := \{X \in \mathbb{K}^n \tc x_1 + \dots + x_n = 0\}\]
	allora:
	\begin{itemize}
		\item $\underline{0} \in \mathbb{W}$
		\item $\mathbb{W}$ è chiuso per la somma
		\item $\mathbb{W}$ è chiuso per il prodotto per uno scalare
	\end{itemize}
	allora $\mathbb{W}$ è un sottospazio. \newline	
	Sia $\mathbb{U} \subset \mathbb{K}^n$, un sottoinsieme, dato da:
	\[\mathbb{U} := \{X \in \mathbb{K}^n \tc x_1 + \dots + x_n = 1\}\]
	allora non è un sottospazio di $\mathbb{K}$, perchè $\underline{0} \not \in \mathbb{U}$
	
	\section{Polinomi su $\mathbb{K}$}
	Un polinomio su $\mathbb{K}$ nella variabile x è espresso come:
	\[a_0x^d + a_1 x^{d -1} + \dots + a_{d -1} x + a_d, a_i \in \mathbb{K}\]
	\[\mathbb{K}[x] := \{\text{polinomi in x a coefficienti in } \mathbb{K}\}\]
	La somma tra polinomi in $\mathbb{K}$ si ottiene sommando i coefficienti:
	\[(x^3 + x +1) + (x^2 - 4x + 3) = x^3 + x^2 -3x + 4\]
	Il prodotto, moltiplicando i coefficienti e sommando i gradi delle x:
	\[(x^3 + x +1)(x^2 - 4x + 3) = x^5 -4x^4 + 4x^3 -3x^2 -x + 3\]
	$\mathbb{K}[x]$ con le operazioni definite è un anello, in cui l'elemento neutro è il polinomio 0, e l'unità moltiplicativa è il polinomio 1.
	
	Sia $p \in \mathbb{K}[x]$ un polinomio a coefficienti in $\mathbb{K}[x]$, del tipo $a_0x^d + a_1 x^{d -1} + \dots + a_{d -1} x + a_d, a_i \in \mathbb{K}$, definiamo una applicazione:
	\[
	\begin{array}{c c c}
		\mathbb{K} & \xlongrightarrow{F_p (\text{ funzione polinomiale})} & \mathbb{K} \\
		c & \longmapsto & p(c) \\
	\end{array}
	\]
	\[p(c) = a_0c^d + a_1 c^{d -1} + \dots + a_{d -1} c + a_d, a_i \in \mathbb{K}\]
	Dove $F_p := \mathbb{Z}/p$, è un campo. \newline
	Se $||\mathbb{K}|| = +\infty$ e si hanno due polinomi $p, q \in \mathbb{K}$, allora:
	\[F_p = F_q \Longrightarrow p = q\]
	Se $||\mathbb{K}|| < +\infty$ allora non è vero. \newline
	
	Esempio: consideriamo $F_2[x]$
	\[F_2[x] := \{[0], [1]\}\]
	\[\phi = (x^2 + x) \in F_2[x]\]
	\[
	\begin{array}{c | c}
		F_2 & F_2 \\
		\hline
		\\
		0 & \phi(0) = 0 \\
		\\
		1 & \phi(0) = 0
	\end{array}
	\]
	
	\subsection{Anello polinomiale}
	Considerato l'anello polinomiale di un certo campo, $\mathbb{K}[x]$, con la somma tra polinomi:
	\[
	\begin{array}{c c c}
		\mathbb{K}[x] \times \mathbb{K}[x] & \xlongrightarrow{\text{somma}} & \mathbb{K}[x] \\
		(p, q)& \longmapsto & p + q \\
	\end{array}
	\]
	e prodotto per costante:
	\[
	\begin{array}{c c c}
		\mathbb{K} \times \mathbb{K}[x] & \xlongrightarrow{\text{prodotto}} & \mathbb{K}[x] \\
		(\lambda, q)& \longmapsto & \lambda p  \\
	\end{array}
	\]
	con queste operazioni $\mathbb{K}[x]$ è uno sp. vett./$\mathbb{K}$, allora:
	\[\mathbb{K}[x]_{ \leq d \geq 0} := \{a_0x^d + a_1x^{d - 1} + \dots + a_d \tc a_i \in \mathbb{K}\} \subset \mathbb{K}[x]\]  
	è un sotto spazio vettoriale di $\mathbb{K}[x]$ in cui la somma tra due polinomi di grado al più d, da come risultato un polinomio di grado al più d, invece il prodotto tra una costante ed un polinomio mantiene il grado massimo.
	
	Osservazione:
	
	Sia $\{\mathbb{W}_i\}_{i \in I}$ una collezione di sottospazi di $\mathbb{V}$, allora :
	\[\bigcap_{i \in I} \mathbb{W}_i\]
	è un sottospazio vettoriale di $\mathbb{V}$ \newline
	Dimostrazione:
	\begin{itemize}
		\item dato che $\underline{0} \in \mathbb{W}_i, \forall i \in I \Rightarrow 0 \in \bigcap_{i \in I} \mathbb{W}_i$
		\item se $w_1, w_2 \in \bigcap_{i \in I} \mathbb{W}_i \rightarrow w_1, w_2 \in \mathbb{W}_i, \forall i \in I$ quindi $(w_1 + w_2) \in \mathbb{W}_i, \forall i \in I \Rightarrow (w_1 + w_2) \in \bigcap_{i \in I} \mathbb{W}_i$
	\end{itemize}
	Esempio: Abbiamo visto che se $a_1, a_2, \dots, a_n \in \mathbb{K}$, allora:
	\[\{X \in \mathbb{K}^n \tc (a_1x_1 + a_2x_2 + ... a_nx_n) = 0\}\]
	
	per la proposizione iniziale, l'insieme delle soluzioni $(x_1, x_2, \dots, x_n)$ del sistema di equazioni lineari omogenee:	
	\[
	\left\{
	\begin{array}{l}
		a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = 0 \\
		a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = 0 \\
		\vdots \\
		a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n = 0 \\
	\end{array}
	\right.
	\]
	è un sottospazio vettoriale di $\mathbb{K}^n$, ed equivale all'intersezione tra le soluzioni delle singole equazioni.
	
	\section{Combinazioni lineari}
	Presi $v_1, v_2, \dots, v_n \in \mathbb{V}$, allora $v \in \mathbb{V}$ è una combinazione lineare di $v_1, \dots, v_n$, se esistono $\lambda_1, \dots, \lambda_n$, tali per cui:
	\[v = \lambda_1 v_1 + \dots + \lambda_n v_n\]
	Esempio 1: presi
	\[\underline{e}_1 := (1, 0, 0, \dots, 0) \in \mathbb{K}^n\]
	\[\underline{e}_2 := (0, 1, 0, \dots, 0) \in \mathbb{K}^n\]
	\[\vdots\]
	\[\underline{e}_n := (0, 0, 0, \dots, n) \in \mathbb{K}^n\]
	allora ogni vettore $X \in \mathbb{K}^n$ è una combinazione lineare di $\underline{e}_1, \underline{e}_2, \dots, \underline{e}_n$, quindi:
	\[X = \lambda_1 \underline{e}_1 + \dots + \lambda_n \underline{e}_n = (x_1, \dots, x_n)\]
	rendendo ogni $x_i = \lambda_i$:
	\[X = x_1 \underline{e}_1 + \dots + x_n \underline{e}_n\]
	Esempio 2: Quali vettori in $\mathbb{K}^3$ sono combinazioni lineari di $\underline{e}_1, \underline{e}_2$?
	\[(x_1,x_2,x_3) = \lambda_1 \underline{e}_1 + \lambda_2 \underline{e}_2 = (\lambda_1, \lambda_2, 0) \Rightarrow x_1 = \lambda_1, x_2 = \lambda_2, x_3 = 0\] 
	Esempio generale, quali vettori in $\mathbb{K}^3$ sono combinazioni lineari di:
	\[v := (a_1, a_2, a_3), W := (b_1, b_2, b_3)\]
	
	$(c_1, c_2, c_3) = C \in \mathbb{K}^3$ è combinazione lineare di $V$ e $W \iff $ esiste una soluzione $(\lambda_1, \lambda_2)$ del sistema di equazioni lineari:
	\[
	\left\{
	\begin{array}{l}
		a_1\lambda_1 + b_1\lambda_2 = c_1 \\
		a_2\lambda_1 + b_2\lambda_2 = c_2 \\
		a_3\lambda_1 + b_3\lambda_2 = c_3 \\
	\end{array}
	\right.
	\]
	dove $\lambda_n$ sono le incognite. Posso quindi scrivere che $C = \lambda_1 v + \lambda_2 w$
	
	\section{Sottospazi generati}
	Preso $\mathbb{S} \subset \mathbb{V}$, allora il sottospazio di $\mathbb{V}$ generato da $\mathbb{S}$ è:
	\[\brackets{\mathbb{S}} := \{\lambda_1 v_1 + \dots \lambda_n v_n \tc v_1, \dots, v_n \in \mathbb{S}, \lambda_1, \dots, \lambda_n \in \mathbb{K}\}\]
	ovvero, il sottospazio generato da $\mathbb{S}$ è l'insieme di tutte le combinazioni lineari di vettori presi da $\mathbb{S}$, è il più piccolo sottospazio di $\mathbb{V}$ che contiene i vettori da $v_1$ a $v_n$, e se l'insieme $\mathbb{S}$ è vuoto, allora, $\brackets{\emptyset} = \{\underline{0}\}$ \newline
	Dimostrazione che $\brackets{S}$ sia effettivamente un sottospazio:
	\begin{itemize}
		\item $\underline{0} \in \mathbb{S}$ perchè? 
			\subitem Se $\mathbb{S} = \emptyset \Rightarrow \brackets{\mathbb{S}} = \{\underline{0}\}$
			\subitem se invece $\mathbb{S} \not = \emptyset$, allora $v \in \mathbb{S} \rightarrow v \in \mathbb{S} \rightarrow 0v = \underline{0} \in \mathbb{S} \Rightarrow \underline{0} \in \brackets{\mathbb{S}}$
		\item siano $v, w \in \brackets{\mathbb{S}}, v = \lambda_1 v_1 \dots + \lambda_n v_n, w = \mu_1 w_1 + \dots + \mu_n w_n$, con $v_1, \dots, v_n, w_1, \dots, w_n \in \mathbb{S}$
		\item verificare prodotto rispetto a scalare
	\end{itemize}
	Notazione:
	\begin{itemize}
		\item $S := \{v_1, v_2, \dots, v_n\}$
		\item $\brackets{v_1, \dots, v_n} = \brackets{\{v_1, \dots, v_n\}}$
	\end{itemize}
	Definito $V = V(\mathbb{E}^2)$, allora:
	
	\begin{center}
		\begin{tikzpicture}[scale=1.3, >=stealth]
		
		% Origine
		\coordinate (O) at (0,0);
		
		% Direzioni di r e w
		\coordinate (rdir) at (1.8,2.5);
		\coordinate (wdir) at (2.6,1.0);
		
		% P e Q = multipli dei vettori r e w
		\coordinate (P) at ($(O)!0.6!(rdir)$);
		\coordinate (Q) at ($(O)!0.7!(wdir)$);
		
		% Punto finale U = P + Q (somma dei vettori)
		\coordinate (U) at ($(P)+(Q)$);
		
		% Rette direzionali
		\draw[thin, gray!50] ($(rdir)!-0.5!(O)$) -- ($(rdir)!1.2!(O)$) node[near end, left] {$v$};
		\draw[thin, gray!50] ($(wdir)!-0.5!(O)$) -- ($(wdir)!1.2!(O)$) node[near end, right] {$w$};
		
		% Vettori OP e OQ
		\draw[->, thick] (O) -- (P) node[midway, left] {$\lambda v$};
		\draw[->, thick] (O) -- (Q) node[midway, below right] {$\mu w$};
		
		% Parallelogramma della somma
		\draw[dashed] (P) -- (U);
		\draw[dashed] (Q) -- (U);
		
		% Vettore risultante u = OP + OQ
		\draw[->, very thick, blue] (O) -- (U) node[midway, above right] {$u$};
		
		% Punti
		\fill (O) circle (1.4pt) node[below left] {$O$};
		\fill (P) circle (1.4pt) node[left] {$P$};
		\fill (Q) circle (1.4pt) node[below right] {$Q$};
		\fill (U) circle (1.4pt) node[above right] {$U$};
		
		% Angolo tra r e w
		\draw pic["$\theta$", draw=black, angle radius=10, angle eccentricity=1.4]
		{angle = wdir--O--rdir};
		
		% Formula descrittiva
		\node[align=left, anchor=west] at (4,2.3) {
			$\vec{OP}=\lambda v,\quad \vec{OQ}=\mu w$\\[4pt]
			$u = \vec{OP} + \vec{OQ} = \vec{OU}$
		};
		
	\end{tikzpicture}
	\end{center}
	$\brackets{v} = \lambda_1 v$ si intendono tutti i vettori paralleli alla retta r, e con origine in 0 . \newline
	$\brackets{v,w} = \lambda_1 v + \lambda_2 w = V(\mathbb{E})$, se i due vettori sono linearmente indipendenti, ovvero non paralleli, quindi ogni vettore nel piano euclideo può essere rappresentato come una combinazione lineare tra $v$ e $w$, se invece i due vettori fossero paralleli, il sottospazio generato sarebbe uguale al sottospazio generato da uno solo dei due vettori.
	
	Osservazione:
	\[S \subset T \subset V \Rightarrow \brackets{S} \subset \brackets{T}\]
	
	\subsection{Sottospazio finitamente generato}
	$V$ è finitamente generato se:
	\[\exists v_1, \dots, v_n \tc \brackets{v_1, \dots, v_n} = V\]
	quindi si dice che V è finitamente generato se esistono finiti vettori tali per cui il sottospazio generato dalle loro combinazioni lineari genera lo spazio vettoriale V.
	
	Ad esempio $\mathbb{K}^n$ è finitamente generato perchè può essere ottenuto attraverso le combinazioni lineari di $\underline{e}_1, \dots,\underline{e}_n$, quindi $\brackets{\underline{e}_1, \dots,\underline{e}_n} = \mathbb{K}^n$
	
	Invece $\mathbb{K}[x]$, ovvero l'anello polinomiale, non è finitamente generato, infatti se $p_1, \dots, p_n \in \mathbb{K}^n$, allora ogni $q \in \brackets{p_1, \dots, p_n}$ ha grado al più il massimo del $\deg p_1, \dots, \deg p_n$
	
	In quanto:
	\[\brackets{p_1, \dots, p_n} = \lambda_1 p_1 + \lambda_2 p_2 +\dots + \lambda_n p_n\]
	
	sommando dei polinomi moltiplicati per una certa costante, non permette di aumentare il grado del polinomio.
	
	Se $V(\mathbb{E}^2)$ è finitamente generato, allora ogni sottospazio di $V(\mathbb{E}^2)$ è finitamente generato.
	
	Ad esempio: in sistema di m equazioni lineari omogenee in n incognite, l'insieme delle soluzioni è un sottospazio vettoriale di $\mathbb{K}^n$, allora $W$ è finitamente generato.
	
	\[
	\left\{
	\begin{array}{l}
		a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = 0 \\
		a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = 0 \\
		\vdots \\
		a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n = 0 \\
	\end{array}
	\right.
	\]
	
	\section{Lista di vettori}
	Per lista di vettori di $\mathbb{V}$ si intende un insieme ordinato di elementi univoci, e può essere descritta da una funzione:
	\[f : \mathbb{I} \longrightarrow \mathbb{V}, \mathbb{I} = [n], n \in N\]
	\[f : f(i) = \mathbb{V}_i\]
	\[f : \{V_i\}_{i \in \mathbb{I}}\]
	
	\subsection{Lista di generatori}
	Una lista di vettori, $v_1, \dots, v_n$, si dice generatrice se:
	\[\forall v \in \mathbb{V}, \exists x_1, \dots, x_n \tc v = \sum_{i = 1}^{n} x_i v_i = x_1 v_1 + x_2 v_2 + \dots + x_n v_n\]
	Quindi i vettori di una lista si dicono generatori se ogni vettore di $\mathbb{V}$ può essere ottenuto per combinazione lineare dei vettori appartenenti alla lista.
	
	Ad esempio, in $\mathbb{R}^2$:
	\begin{itemize}
		\item $
			\begin{pmatrix}
				1 \\
				0
			\end{pmatrix}, \begin{pmatrix}
			0 \\
			1
			\end{pmatrix} 
			\rightarrow
			\begin{pmatrix}
				a \\
				b
			\end{pmatrix} = 
			a
			\begin{pmatrix}
			1 \\
			0
			\end{pmatrix} + 
			b
			\begin{pmatrix}
			0 \\
			1
			\end{pmatrix} 
			$
			
			quindi in questo caso ho solo un modo per scrivere un vettore $\begin{pmatrix}
				a \\
				b
			\end{pmatrix}$, la lista quindi è generatrice e genera i vettori in modo univoco
			\item $\begin{pmatrix}
				1 \\
				0
			\end{pmatrix}, 
			\begin{pmatrix}
				1 \\
				1
			\end{pmatrix},
			\begin{pmatrix}
				-1 \\
				1
			\end{pmatrix} \rightarrow \begin{pmatrix}
			a \\
			b
			\end{pmatrix} = 
			(a - b) \begin{pmatrix}
			1 \\
			0
			\end{pmatrix} + b \begin{pmatrix}
			1 \\
			1
		\end{pmatrix} = 
			(a + b) \begin{pmatrix}
			1 \\
			0
		\end{pmatrix} + b \begin{pmatrix}
			-1 \\
			1
		\end{pmatrix}$
		
		in questo caso la lista è sempre generatrice, ma ho più modi per scrivere lo stesso vettore, e ciò deriva dal fatto che non sono \textbf{linearmente indipendenti}, infatti:
		\[\begin{pmatrix}
			-1 \\
			1
		\end{pmatrix} = \begin{pmatrix}
		1 \\
		1
		\end{pmatrix} -2
		\begin{pmatrix}
			1 \\
			0
		\end{pmatrix}\]
	\end{itemize}
	
	Quindi una lista è generatrice se per ogni vettore $v$, esiste almeno una scelta di $x_1, \dots, x_n$, che soddisfa la relazione di dipendenza lineare di indice v. 
	
	\subsection{Lista di vettori linearmente indipendenti}
	Una lista di vettori, $v_1, \dots, v_n$, si dice \textbf{linearmente indipendente} se l’unica soluzione dell’equazione di dipendenza lineare:
	\[x_1 v_1 + \dots + x_n v_n = 0\]
	è la soluzione banale, cioè quella in cui tutti i coefficienti sono nulli:
	\[x_1 = x_2 = \dots = x_n = 0\]
	In tal caso:
	\[0v_1 + 0v_2 + \dots + 0v_n = 0\]
	che rappresenta l’\textbf{equazione banale}.
	
	Se invece esiste una combinazione non banale (cioè con almeno un coefficiente $x_i \not = 0$) che dà lo zero vettoriale, allora la lista si dice linearmente dipendente.
	
	Ad esempio, in $\mathbb{R}^3$
	\begin{itemize}
		\item $\begin{pmatrix}
			1 \\
			0 \\
			0
		\end{pmatrix},
		\begin{pmatrix}
			0 \\
			1 \\
			0
		\end{pmatrix},
		\begin{pmatrix}
			0 \\
			0 \\
			1
		\end{pmatrix} \rightarrow \begin{pmatrix}
			0 \\
			0 \\
			0
		\end{pmatrix} = x_1 \begin{pmatrix}
			1 \\
			0 \\
			0
		\end{pmatrix} + x_2 
		\begin{pmatrix}
			0 \\
			1 \\
			0
		\end{pmatrix} + x_3
		\begin{pmatrix}
			0 \\
			0 \\
			1
		\end{pmatrix} \Rightarrow x_1 = x_2 = x_3 = 0$
		
		Sono quindi linearmente indipendenti, ed inoltre sono generatori, in quanto è possibile ottenere ogni vettore di $\mathbb{R}^3$ attraverso una combinazione lineare di questi 3 vettori.
		
		Questa lista di vettori è detta la forma canonica/standard di $\mathbb{R}^3$, in quanto è la lista più "semplice" a generare tutto l'insieme. 
		
		\item $\begin{pmatrix}
			1 \\
			0 \\
			1
		\end{pmatrix},
		\begin{pmatrix}
			1 \\
			2 \\
			1
		\end{pmatrix},
		\begin{pmatrix}
			1 \\
			1 \\
			1
		\end{pmatrix} \rightarrow 
		\begin{pmatrix}
			0 \\
			0 \\
			0
		\end{pmatrix} = 
		2\begin{pmatrix}
			1 \\
			1 \\
			1 
		\end{pmatrix} - 
		\begin{pmatrix}
			1 \\
			0 \\
			1
		\end{pmatrix} - 
		\begin{pmatrix}
			1 \\
			2 \\
			1
		\end{pmatrix} \Rightarrow x_1 = 2, x_2 = x_3 = -1$.
		
		Quindi questa lista di vettori non è linearmente indipendente, ed inoltre è possibile scrivere il secondo vettore come combinazione lineare degli altri due:
		\[
		\begin{pmatrix}
		1 \\
		2 \\
		1
		\end{pmatrix} = 2\begin{pmatrix}
		1 \\
		1 \\
		1
		\end{pmatrix} -
		\begin{pmatrix}
		1 \\
		0 \\
		1
		\end{pmatrix}\]
		In questo caso, i vettori $(1,1,1), (1,0,1)$, generano un piano, che contiene il vettore $(1,2,1)$.
	\end{itemize}
	
	
	Quindi una lista è linearmente indipendente se per ogni vettore $v$, esiste al massimo una scelta di $x_1, \dots, x_n$, che soddisfa la relazione di dipendenza lineare di indice v.
	
	Se una lista di vettori è linearmente indipendente, allora ogni sottolista è linearmente indipendente.
	
	\subsection{Lista di vettori base}
	Una lista di vettori, $v_1, \dots, v_n$, è una base se la lista è allora stesso tempo:	
	\begin{enumerate}
		\renewcommand{\labelenumi}{\textit{(\roman{enumi})}}
		\item composta da vettori generatori.
		\item linearmente indipendente.
	\end{enumerate}
	Esempio:
	\begin{itemize}
		\item $\begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix}, \begin{pmatrix}
		1 \\ 1 \\ 1
		\end{pmatrix}$
		
		Controllo che siano linearmente indipendenti:
		\[\begin{pmatrix} 0 \\ 0\\ 0\end{pmatrix} = 
		x_1\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} + 
		x_2\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} =
		\begin{pmatrix} x_1 + x_2 \\ x_2 \\ x_1 + x_2 \end{pmatrix} \Rightarrow x_2 = 0, x_1 + x_2 = 0 \Rightarrow x_1 = x_2 = 0
		\]
		L'unico modo in cui si può ottenere il vettore 0 è se $x_1 = x_2 = 0$, quindi la soluzione all'equazione di dipendenza lineare è l'equazione banale, quindi sono linearmente indipendenti.
		
		Controllo che sia una lista generatrice:
		\[\begin{pmatrix} 1 \\ 0 \\ 0\end{pmatrix} = x_1 \begin{pmatrix} 1\\ 0 \\1 \end{pmatrix} + x_2 \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} x_1 + x_2 \\ x_2 \\ x_1 + x_2 \end{pmatrix}\]
		Da cui deriva:
		\[\left\{
		\begin{aligned}
			x_1 & + x_2 = 1 \\
			x_2 & = 0 \\
			x_1 & + x_2 = 0
		\end{aligned}
		\right.\]
		Il sistema è inconsistente in quanto $x_1 + x_2$ deve fare 1 e 0 allo stesso tempo, quindi questo vettore non è ottenibile come combinazione lineare degli elementi nella lista, quindi:
		\[\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \not \in \brackets{\begin{pmatrix} 1 \\ 0 \\ 1\end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix}}\]
		\item $\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$
		
		Controllo che siano linearmente indipendenti, e non lo sono in quanto posso scrivere il terzo vettore come combinazione lineare dei primi 2:
		\[\begin{pmatrix} 1 \\ 2 \\ 1\end{pmatrix} = 2\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} - \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}\]
		Dato che il vettore $\begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}$ può essere scritto come combinazione lineare, allora lo spazio generato dalla lista iniziale è equivalente a:
		\[\brackets{\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}} = \brackets{\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}}\]
		
		Ora controllo che siano generatori, verificando se un vettore generico possa essere scritto come combinazione lineare degli elementi della lista:
		\[\begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix} = x_1 \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} + x_2 \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} + x_3 \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} x_1 + x_2 + x_3 \\ x_2 \\ x_1 + x_2 \end{pmatrix}\]
		\[\left\{\begin{aligned}
			x_1 & + x_2 + x_3 = b_1 \\
			x_2 & = b_2 \\
			x_1 & + x_2 = b_3
 		\end{aligned}\right. \Rightarrow \left\{\begin{aligned}
 		x_3 & = b_1 - b_3 \\
 		x_2 & = b_2 \\
 		x_1 & = b_3 - b_2
 		\end{aligned}\right.\]
 		\[\forall b_1, b_2, b_3, \exists x_1, x_2, x_3 \tc \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix} \in \brackets{\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}}\]
 		Quindi la lista è generatrice ma non linearmente indipendente.
 		
		\item $\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$
		
		Ora basta controllare che siano linearmente indipendenti in quanto abbiamo già dimostrato sopra che sono generatori, quindi controllo che l'unica soluzione possibile per l'equazione di dipendenza lineare sia quella in cui tutti i coefficienti sono uguali a 0:
		\[ \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix} = x_1 \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} + x_2 \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} + x_3 \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} x_1 + x_2 + x_3 \\ x_2 \\ x_1 + x_2 \end{pmatrix}\]
		\[\left\{\begin{aligned}
			x_1 & + x_2 + x_3 = 0 \\
			x_2 & = 0 \\
			x_1 & + x_2 = 0
		\end{aligned}\right. \Rightarrow x_1 = x_2 = x_3 = 0\]
		Quindi la lista è linearmente indipendente.
	\end{itemize}
	
	Quindi una lista è base se per ogni vettore $v$, esiste ed è unica la scelta di $x_1, \dots, x_n$, che soddisfa la relazione di dipendenza lineare di indice v.
	
	\subsection{Lemma di unicità della rappresentazione}
	Sia $v_1, \dots, v_n$ una lista di vettori indipendenti, allora:
	\[v = x_1 v_1 + \dots x_n v_n = y_1 v_1 + \dots y_n v_n\]
	\[\Rightarrow x_1 = y_1, \dots, x_n = y_n\]
	Dimostrazione:
	\[v - v = x_1 v_1 + \dots + x_n v_n - y_1 v_1 - \dots y_n v_n\]
	\[\Rightarrow 0 = (x_1 - y_1) v_1 + \dots + (x_n - y_n) v_n\]
	\[\Rightarrow x_1 = y_1, \dots x_n = y_n\]
	Quindi se si ha una lista indipendente allora ho un solo modo univoco per rappresentare un vettore come combinazione lineare di altri vettori.
	
	\subsection{Lista polinomiale}
	Preso l'insieme dei polinomi in $x$ di grado massimo $d$ a coefficienti in $\mathbb{K}$:
	\[\mathbb{K}_{\leq d} [x] = \{a_0 + a_1 x + \dots + a_d x^d \tc a_i \in \mathbb{K}\}\]
	Allora:
	\[1, x, x^2, \dots, x^d\]
	è una base, nello specifico la base canonica di $\mathbb{K}_{\leq d} [x]$, preso $v \in \mathbb{K}_{\leq d} [x]$, allora:
	\[v = a + a_1 x + \dots + a_d x^d = \lambda_0 + \lambda_1 x + \dots + \lambda_d x^d\]
	L'unica soluzione è che $\lambda_i = a_i$, allora dato che se due polinomi sono uguali, la differenza tra i coefficienti è nulla:
	\[a_0 + \dots + a_d x^d = b_0 + \dots + b_d x^d \iff a_i = b_i\]
	\[(a_0 - b_0) + \dots + (a_d - b_d)x^d = 0\]
	
	\section{Matrici}
	Presa una matrice 2x2:
	\[Mat_{2x2}(\mathbb{K}) := \left\{\begin{pmatrix} a & b \\ c & d\end{pmatrix} \tc a,b,c,d \in \mathbb{K}\right\}\]
	Con le operazioni di somma e prodotto per scalare:
	\begin{itemize}
		\item Somma:
		\[\begin{pmatrix} a_1 & b_1 \\ c_1 & d_1 \end{pmatrix} + \begin{pmatrix} a_2 & b_2 \\ c_2 & d_2 \end{pmatrix} = 
		\begin{pmatrix} a_1 + a_2 & b_1 + b_2 \\ c_1 + c_2 & d_1 + d_2 \end{pmatrix}\]
		\item Prodotto per scalare:
		\[k\begin{pmatrix} a & b \\ c & d \end{pmatrix} = \begin{pmatrix} ka & kb \\ kc & kd \end{pmatrix}\]
	\end{itemize}
	è uno spazio vettoriale in quanto contiene l'elemento neutro della somma, è chiuso per la somma e per il prodotto per uno scalare.
	
	La base canonica di una matrice 2x2 è:
	\[\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}\]
	infatti, ogni matrice 2x2 può essere scritta come combinazione lineare della base canonica:
	\[\begin{pmatrix} a & b \\ c & d \end{pmatrix} = x_1 \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} + x_2 \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} + x_3 \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} + x_4 \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}\]
	\[\Rightarrow x_1 = a, x_2 = b, x_3 = c, x_4 = d\]
	
	\section{Dimensione di uno spazio vettoriale}
	Per dimensione di un certo spazio vettoriale $\mathbb{V}$, si intende il numero degli elementi di una base di $\mathbb{V}$, e si denota con:
	\[\dim \mathbb{V}\]
	Ad esempio:
	\begin{itemize}
		\item $\dim \mathbb{R}^3 = 3$
		\item $\dim \mathbb{R}^n = n \longrightarrow n = \left|\left\{e_1 = \begin{pmatrix} 1 \\ 0 \\ \vdot \\ 0 \end{pmatrix}, e_2 = \begin{pmatrix} 0 \\ 1 \\ \vdot \\ 0 \end{pmatrix}, \dots , e_n = \begin{pmatrix} 0 \\ \vdot  \\ 0 \\ 1 \end{pmatrix}\right\}\right|$, ovvero la base canonica di ogni vettore in $\mathbb{R}^n$
		\item $\dim (\mathbb{K}_{\leq d} [x]) = d + 1 \longrightarrow (1 + x + x^2 + \dots + x^d)$, ovvero la base canonica di ogni polinomio di grado massimo d
		\item Presa una matrice 2x2 del tipo:
		\[M_{2x2} = \{\begin{pmatrix} a & b \\ c & d \end{pmatrix} \tc a,b,c,d \in \mathbb{K}\}\]
		allora la base canonica di una matrice due per due è:
		\[\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}\]
		quindi:
		\[\dim M_{2x2} = 4\]
	\end{itemize}
	Ogni lista \textbf{linearmente indipendente}, ha al massimo $\dim \mathbb{V}$ elementi, dove $\mathbb{V}$ è uno spazio vettoriale.
	
	Invece ogni lista di \textbf{generatori}, ha almeno $\dim \mathbb{V}$ elementi.
	
	Si dice sottospazio proprio finitamente generato un sottospazio diverso dall'intero spazio vettoriale e dal vettore nullo, che ha quindi come dimensione massima, la dimensione dell'intero spazio vettoriale meno uno, ad esempio:
	\[V \subseteq \mathbb{K}^n \Rightarrow \dim \mathbb{K}^n = n \rightarrow \dim V < \dim \mathbb{K}^n \Rightarrow \dim V \leq n - 1\]
	
	\section{Teorema di Steinitz / dello scambio}
	\teorem{Se $v_1, \dots, v_n = (\{v_i\}_{i \in \mathbb{I}})$, è una lista indipendente, e $w_1, \dots, w_m = (\{w_j\}_{j \in \mathbb{G}})$ è una lista di generatori, allora:
	\begin{itemize}
		\item $n \leq m \Rightarrow |\mathbb{I}| \leq |\mathbb{G}|$
		\item Dopo aver riordinato, è possibile cambiare $n$ vettori di $w_j$, con $v_1, \dots, v_n$ e il risultato genera ancora
	\end{itemize}}
	
	Ad esempio:
	\[\exists \mathbb{G}^\prime \subset \mathbb{G}, |\mathbb{G}^\prime| = n * m \tc \] 
	\[ \{v_i, w_j\}_{i \in \mathbb{I}, j \in \mathbb{G}}\]
	\[\mathbb{I} := \left\{\begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix}\right\}\]
	\[\mathbb{G} := \left\{\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\right\}\]
	Posso mettere $\begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix}$ al posto di $\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$, e $\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$ al posto di $\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$, ottenendo:
	\[\mathbb{G} := \left\{\begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}\right\}\]
	Ma questa cosa non vale sempre, infatti bisogna scegliere con cura i vettori.
	
	\subsection{Dimostrazione teorema di Steinitz}
	INSERIRE DIMOSTRAZIONE PER INDUZIONE DAL MANUALE.
	
	\subsection{Conseguenze del teorema}
	\begin{itemize}
		\item Ogni base ha lo stesso numero di elementi.
	
			Dimostrazione:
			\[\{v_i\}_{i \in \mathbb{B}} \wedge \{w_j\}_{j \in \mathbb{B}^\prime}\]
			Supponendo che siano basi, dove $\mathbb{V}_i$ è indipendente e anche generatrice, e $\mathbb{W}_i$ è generatrice e anche indipendente.
			
			Quindi la cardinalità di $\mathbb{B}$ è minore o uguale a quella di $\mathbb{B}^\prime$, oppure $\mathbb{B}^\prime$ è minore o uguale a quella di $\mathbb{B}$, ciò implica che le due cardinalità siano uguali.
			\[\left.
				\begin{aligned}
				|\mathbb{B}| \leq |\mathbb{B}^\prime| \\
				|\mathbb{B}^\prime| \leq |\mathbb{B}|
				\end{aligned}
			\right\} \Rightarrow |\mathbb{B}| = |\mathbb{B}^\prime|\]
		
		\item Lemma: Ogni lista $\{v_i\}_{i \in \mathbb{I}}$ indipendente massima (quindi con cardinalità uguale alla $\dim \mathbb{V}$) è una base:
		\[|\mathbb{I}| = \dim \mathbb{V}\]
		Ogni lista $\{w_j\}_{j \in \mathbb{G}}$ generatori minima, (quindi con cardinalità uguale alla $\dim \mathbb{V}$) è una base:
		\[|\mathbb{G}| = \dim \mathbb{V}\]
		
		La dimensione è il più importante invariante di uno spazio vettoriale. %%TO DO
		
		Dimostrazione del primo punto del lemma:
		\[\brackets{v_i}_{i \in I} \subset \mathbb{V}\]
		per definizione è un sottospazio.
		\begin{itemize}
			\item Se $\brackets{v_i}_{i \in I} = \mathbb{V}$, allora i vettori sono una base.
			\item Se $\brackets{v_i}_{i \in I} \not = \mathbb{V}$, allora $\exists v \not \in \brackets{v_i}_{i \in I}$
		\end{itemize}
		
		Presa la lista:
		\[\{v_i, v\}_{i \in \mathbb{I}}\]
		allora se si vuole dimostrare la seconda definizione:
		\[\sum_{i \in \mathbb{I}} x_i v_i + x v = 0\]
		con $x \not = 0$, però:
		\[v = \frac{1}{x}(- \sum_{i \in \mathbb{I}} x_i v_i)\]
		Si arriva ad una contraddizione in quanto si era posto che la lista indipendente fosse massima, perciò deve per forza essere indipendente.
		
		Dimostrazione del secondo punto del lemma:
		\begin{itemize}
			\item $\{w_j\}_{j \in \mathbb{G}}$ è indipendente.
			\item $\{w_j\}_{j \in \mathbb{G}}$ non è indipendente.
		\end{itemize}
		Posso allora scrivere l'equazione di dipendenza lineare:
		\[\sum_{j \in \mathbb{G}} y_j w_j = 0\]
		con almeno un $y_j \not = 0$, e lo chiameremo $y_k$
		\[\Rightarrow w_k = \frac{1}{y_k}(- \sum_{j \in \mathbb{G}} \setminus \{k\}) y_j w_j \Rightarrow w_k \in \brackets{w_j}_{j \in \mathbb{G} \setminus \{k\}}\]
		e questo contraddice il fatto che la lista $\{w_j\}_{j \in \mathbb{G}}$ sia minima.
		
		Conseguenze del lemma:
		\begin{itemize}
			\item Ogni lista $\{v_i\}_{i \in \mathbb{I}}$ di vettori indipendenti si può estendere a una base, aggiungendo vettori indipendente finché la lista non diventa massima
			\item Ogni lista $\{w_j\}_{j \in \mathbb{G}}$ di generatori contiene una base, è sempre presenta una sottolista di generatori minima
		\end{itemize}
		\item Corollario: Se $\mathbb{W} \subset \mathbb{V}$, $\dim \mathbb{W} <= \dim \mathbb{V}$ è in realtà $\dim \mathbb{W} = \dim \mathbb{V} \iff \mathbb{V} = \mathbb{W}$
		
		Dimostrazione: presa una base di $\mathbb{W}$:
		\[\brackets{\{v_i\}_{i \in \mathbb{B}_w}} = \mathbb{W}\]
		quindi $|\mathbb{B}_w| = \dim \mathbb{W}$
		Allora i $v_i$ sono per definizione indipendenti, quindi:
		\[|\mathbb{B}_w| \leq \dim \mathbb{V}\]
		se $\mathbb{W} \subset \mathbb{V}$, allora è possibile estendere $\{v_i\}_{i \in \mathbb{I}}$ ad una base di $\mathbb{V}$ e quindi $|\mathbb{B}_w| < \dim \mathbb{V}$
	\end{itemize}
	
	\section{Formula di Grassman}
	Sia $\mathbb{V}$ uno spazio vettoriale su $\mathbb{K}$, e $\mathbb{U}, \mathbb{W} \subset \mathbb{V}$ sottospazi finitamente generati, allora:
	\[\dim (U + W) = \dim U + \dim W - \dim (U \cap W)\] 
	
	Ad esempio:
	\begin{itemize}
		\item In $\mathbb{R}^3 = V$, allora siano $U, W$ due piano, quindi $\dim U = \dim W = 2$, allora:
		\[\dim U \cap W = \left\{\begin{aligned}
			1 & \\
			2 & \iff U = W
		\end{aligned}\right.
		\]
		\item In $\mathbb{R}^4$, con $U, W$ piani, allora $U, W \leq W + U \leq \mathbb{R}^4$, allora:
		\[\dim U \cap W = \dim U + \dim W - \dim (U + W) = \left\{\begin{aligned}
			0 & \iff U + W = \mathbb{R}^4 \\
			1 & \\
			2 & \iff U = W
		\end{aligned}\right.\]
		Se $U = \brackets{v_1, v_2}$ e $W = \brackets{w_1, w_2}$, allora nel sistema otteniamo:
		\begin{itemize}
			\item 0, se $v_1, v_2, w_1, w_2$ sono indipendenti
			\item 1, se uno dei vettori che genera il piano $W$ appartiene al piano $U$, o viceversa
			\item 2, se $v_1, v_2$ generano lo stesso piano di $w_1, w_2$, quindi appartengono allo stesso piano
		\end{itemize} 
	\end{itemize}
	
	\subsection{Dimostrazione}
	Sia la lista $v_1, \dots, v_k$ una base di $U \cap W$, allora la lista ad una  base per $U$, $W$:
	\begin{itemize}
		\item $v_1, \dots, v_k, v_{k+1}, \dots, v_n$, base di $U$
		\item $v_1, \dots, v_k, w_{k+1}, \dots, w_m$, base di $W$
	\end{itemize}
	Affermazione:$v_1, \dots, v_k, v_{k+1}, \dots, v_n, w_{k+1}, \dots, w_m$, è una base per $U + W$, bisogna però dimostrarlo, controllo quindi che siano generatori:
	\[U + W := \{u + w \tc u \in U, w \in W\}\]
	quindi:
	\[u = \sum_{i = 1}^{k} x_i v_i + \sum_{i = k + 1}^{n} y_i u_i\]
	\[w = \sum_{i = 1}^{k} x^1_i v_i + \sum_{i = k + 1}^{m} z_i w_i\]
	\[\Rightarrow u + w = \sum_{i = 1}^{k} x_i v_i + \sum_{i = k + 1}^{n} y_i u_i + \sum_{i = 1}^{k} x^1_i v_i + \sum_{i = k + 1}^{m} z_i w_i\] 
	\[\Rightarrow u + w = \sum_{i = 1}^{k} (x_i + x^1_i)v_i + \sum_{i = k + 1}^{n} y_i u_i + \sum_{i = k + 1}^{m} z_i w_i\]
	Quindi è la lista è generatrice.
	
	Ora controllo l'indipendenza lineare:
	\begin{align}
		\sum_{i = 1}^{k} x_i v_i + \sum_{i = k + 1}^{n} y_i u_i + \sum_{i = k + 1}^{m} z_i w_i = 0 \label{eq:indip1}
	\end{align}
	\[\sum_{i = k + 1}^{m} z_i w_i = - \sum_{i = 1}^{k} x_i v_i - \sum_{i = k + 1}^{n} y_i u_i\]
	Dove:
	\[\sum_{i = k + 1}^{m} z_i w_i \in W\]
	ed anche: 
	\[ - \sum_{i = 1}^{k} x_i v_i - \sum_{i = k + 1}^{n} y_i u_i \in U\]
	quindi:
	\begin{align}
		\sum_{i = 1}^{k} x^1_i v_i = \sum_{i = k + 1}^{n} y_i u_i = - \sum_{i = k + 1}^{m} z_i w_i -\sum_{i = 1}^{k} x_i v_i \label{eq:indip2}
	\end{align}
	\[\Rightarrow \sum_{i = 1}^{k} (x^1_i + x_i) v_i + \sum_{i = k +1 }^{n} y_i u_i\]
	\[\Rightarrow y_{k + 1} = \dots = y_n = 0\]
	
	Quindi \eqref{eq:indip1} diventa:
	\[\sum_{i = 1}^{k} x_i v_i + \sum_{i = k+ 1}^{m} z_i w_i = 0\]
	\[\Rightarrow x_1 = x_2 = \dots = x_k = z_{k-1} = \dots = z_m = 0\]
	perchè: \[\Rightarrow v_1 = \dots = v_k = w_{k+1} = w_m\]
	essendo una base di W allora tutti i vettori sono tutti uguali a 0.
	
	quindi:
	\[v_i, \dots, v_k, w_{k+1} = \dots = w_m\]
	Allora dimensione:
	\[\dim U + W = \text{ num elementi in } v_i, \dots, v_k, w_{k + 1}, \dots, w_m, u_{k + 1}, \dots, u_n\]
	\[\dim U + W = n + m - k = \dim U + \dim W - \dim U \cap W\]
	
	\section{Coordinate associate alla base}
	Perso $V$ un sotto spazio vettoriale su $K$, presi $v_1, \dots, v_n \in V$
	
	Considero l'applicazione:
	\[
	\begin{array}{c c c}
		\mathbb{K}^n & \xlongrightarrow{f} & V \\
		X & \longmapsto & x_1 v_1 + \dots + x_n v_n \\
	\end{array}
	\]
	Allora l'applicazione $f$ è: 
	\begin{itemize}
		\item iniettiva se e solo se $v_1, \dots, v_n$ sono linearmente indipendenti
		\item suriettiva se e solo se $v_1, \dots, v_n$ sono generatori di V
	\end{itemize}
	Quindi se $\mathcal{B}= \{v_i, \dots, v_n\}$ è una base di $V$, allora:
	\[
	\begin{array}{c c c}
		\mathbb{K}^n & \xlongrightarrow{f} & V \\
		X & \longmapsto & \sum_{i = 1}^{n} x_i v_i \\
	\end{array}
	\]
	è biunivoca, perciò ho l'inverso:
	\[
	\begin{array}{c c c}
		V & \xlongrightarrow{f} & \mathbb{K}^n \\
		v & \longmapsto & X_\mathcal{B}(V) = (x_1, \dots, x_n) \\
	\end{array}
	\]
	dove $x_1, v_1 + \dots + x_n v_n = v$ \newline
	Allora si definisce come coordinate di $v$ nella base $\mathcal{B}$ la n-upla dei coefficienti $x_i$ ottenuto come combinazione lineare della base quando la si pone uguale a $v$, quindi $(x_i, \dots, x_n)$
	
	esempio:
	\begin{itemize}
		\item $V = \mathbb{K}^n$, allora $\mathcal{S} = \{e_1, \dots, e_n\}$, dove $e_1 = (1, 0, \dots), \dots, e_n = (0, \dots, 1)$
		\[X = (x_1, \dots, x_n) = x_1 e_1 + \dots + x_n e_n\]
		quindi le coordinate di $X$ nella base standard sono le sue entrare
		\item Presi $v_1 = (1, 1), v_2 = (1, -1)$ entrambi in $ \mathbb{K}^n$ \newline Controllare che: $\{v_1, v_2\}$ sia una base di $\mathbb{K}^n$, controllo che siano linearmente indipendenti, e lo sono se e solo se $2 \not = 0$, perchè: \newline
		\[\lambda_1, \lambda_2 \in \mathbb{K}\]
		Allora:
		\[(0,0) = \lambda_1 v_1 + \lambda_2 v_2 = (\lambda_1 + \lambda_2, \lambda_1 - \lambda_2)\]
		\[\left\{\begin{aligned}
			0 & = \lambda_1 + \lambda_2 \\
			0 & = \lambda_1 - \lambda_2
		\end{aligned}\right. \Rightarrow
		\left\{\begin{aligned}
		0 & = 2\lambda_1 \\
		0 & = 2\lambda_2
		\end{aligned}\right.  \]
		Quindi se e solo se $2 \not = 0$, posso moltiplicare per l'inverso di 2, ovvero $2^{-1}$, ottenendo:
		\[
		\left\{\begin{aligned}
			0 & = \lambda_1 \\
			0 & = \lambda_2
		\end{aligned}\right.  
		\]
		Invece se: $2 = 0$, allora $1 = -1$, e quindi $v_1 = v_2$, rendendoli quindi linearmente dipendenti.
		
		Un campo in cui $n \not = 0$ si dice di caratterizzazione n.
		
		Inoltre la lista di vettori è anche generatrice, ed è banale la dimostrazione, quindi la lista è una base.	
		
		Dato un certo vettore $v = (a,b)$, quali sono le coordinate di v nella base $\mathcal{B}$, ovvero $X_\mathcal{B}(V)$?
		Come già detto le coordinate di un vettore in una carta base sono i coefficienti di vettori della base quando si scrive $v$ come combinazione lineare della base:
		\[(a,b) = x_1 (1,1) + x_2 (1,-1)\]
		\[(a,b) = (x_1 + x_2, x_1 - x_2)\]
		\[\left\{\begin{aligned}
			a & = x_1 + x_2 \\
			b & = x_1 - x_2 
		\end{aligned}\right. \Rightarrow
		\left\{\begin{aligned}
			2x_1 & = a + b \\
			2x_2 & = a - b 
		\end{aligned}\right. \Rightarrow 
		\left\{\begin{aligned}
		x_1 & = 2^{-1}(a + b) \\
		x_2 & = 2^{-1}(a - b) 
		\end{aligned}\right. \iff 2 \not = 0\]
		Quindi le coordinate di v sono:
		\[X_\mathcal{B}(v) = (2^{-1}(a + b), 2^{-1}(a - b))\]
		
		\item Sia $V \subset \mathbb{K}$, definito da:
		\[V := \{X \tc x_1 + \dots + x_n = 0\}\]
		Allora $V$ è un sottospazio vettoriale di $\mathbb{K}^n$, quindi è un sp. vett/$\mathbb{K}$. Si possono trovare le basi di V, ad esempio:
		\begin{itemize}
			\item $\mathcal{B} = \{(1, -1, 0, \dots, 0), (0, 1, -1, 0 , \dots, 0), \dots, (0, \dots, 0, 1, -1)\}$
			\item $\mathcal{C} = \{(1, -1, 0, \dots, 0), (1, 0, -1, 0, \dots, 0), \dots, (1, 0, \dots, 0, -1)\}$
		\end{itemize}
		essendo $V$ un sottospazio proprio di $\mathbb{K}^n$, allora le il numero di vettori, w, nella base sarà n-1, ma ogni vettori avrà n entrate.
		
		Quindi $v = (x_1, \dots, x_n)$, con $\sum_{i = 1}^{n} x_i = 0 \Rightarrow x_1 = - x_2 - x_3 \dots - x_n$
		
		Allora:
		\[(x_1, \dots, x_n) = \lambda_1 w_1 + \dots + \lambda_{n-1} w_{n-1} = (\lambda_1 + \dots + \lambda_{n-1}, -\lambda_1, \dots, \lambda_{n-1})\]
		Questo implica che $(x_1, \dots, x_n) = \lambda_1 w_1 + \dots + \lambda_{n-1} w_{n-1}$ è uguale a:
		\[\left\{\begin{aligned}
			-x_2 & - \dots - x_n = \lambda_1 + \dots + \lambda_{n-1} \\
			x_2 & = -\lambda_1 \\
			x_3 & = -\lambda_2 \\
			\vdots & = \vdots \\
			x_n & = - \lambda_{n-1}
		\end{aligned}\right. \Rightarrow
		\left\{\begin{aligned}
			-x_2 & - \dots - x_n = \lambda_1 + \dots + \lambda_{n-1} \\
			\lambda_1 &  = - x_2 \\
			\lambda_2 &  = - x_3 \\
			\vdots & = \vdots \\
			\lambda_{n-1} &  = - x_n 
		\end{aligned}\right. 
		\]
		Quindi $\mathcal{C}$ è una base di $V$, e le coordinate nella base e di v sono:
		\[X_\mathcal{C}(v) = X_\mathcal{C}((x_1, x_2, \dots, x_n)) = (-x_2, -x_3, \dots, -x_n)\]
	\end{itemize}
	
	\chapter{Applicazioni Lineari}
	Siano $V, W$ sp. vettoriali su $\mathbb{K}$, allora si definisce una applicazione lineare:
	\[
		V \xlongrightarrow{f} W
	\]
	se:
	\begin{itemize}
		\item $\forall v_1, v_2 \in V \Rightarrow f(v_1 + v_2) = f(v_1) + f(v_2)$
		\item $\forall v \in V, \lambda \in \mathbb{K} \Rightarrow f(\lambda v) = \lambda f(v)$
	\end{itemize}
	Ad esempio:
	\[
	\begin{array}{c c c c}
		\mathbb{R} & \xlongrightarrow{f} & \mathbb{R} \\
		x & \longmapsto & a_n x^n + \dots + a_0 & a_n \not = 0, n_{n - 1} \dots, a_0 \in \mathbb{R}
	\end{array}
	\]
	è lineare se e solo se:
	\begin{itemize}
		\item $n = 1, a_0 = 0$
		\item $n = 0, a_0 = 0$
	\end{itemize}
	
	Quindi visto graficamente è una applicazione lineare se e solo se è una retta non verticale che passa per l'origine, o la retta dell'asse x.
	
	Verifico quindi che $f: \mathbb{R} \longrightarrow \mathbb{R}, f(x) = \mu x$ sia lineare:
	\begin{itemize}
		\item $f(x + y) \overset{?}{=} f(x) + f(y), x, y \in \mathbb{R}$
		\[\mu (x+y) \overset{?}{=} \mu x + \mu y\]
		\[\mu (x+y) = \mu x + \mu y\]
		per la proprietà distributiva del prodotto rispetto alla somma.
		\item $f(\lambda x) \overset{?}{=} \lambda f(x), x \, \lambda in \mathbb{R}$
		\[\mu (\lambda x) = \lambda (\mu x)\]
		per la proprietà associativa del prodotto
	\end{itemize}
	
	\teorem{Presi $V, W$ sp.vettori su $\mathbb{K}$, sia f una applicazione lineare:
	\[V \xlongrightarrow{f} W\]
	allora:
	\begin{enumerate}
		\item $f(0_v) = 0_w$
		\item $f(-v) = -f(v)$
		\item $f(\lambda_1 v_1 + \dots + \lambda_n v_n) = \lambda_1 f(v_1) + \dots + \lambda_n f(v_n)$
	\end{enumerate}
	}
	
	Dimostrazione:
	\begin{enumerate}
		\item $f(0_v) = f(0_v + 0_v) \overset{\text{f lin.}}{=} f(0_v) + f(0_v)$
		\[\Rightarrow 0_w = f(0_v) + (-f(0_v)) = f(0_v) + f(0_v) + (-f(0_v)) = f(0_v) + 0_w = f(0_v)\]
		\item $f(-v) = f((-1)v) \overset{\text{f lin.}}{=} f(-1) * f(v) = -f(v)$
		\item Si dimostra per induzione su n, la base induttiva è già verificata per induzione, allora dimostro il passo induttivo assumendo n vera, dimostrando quindi n + 1:
		\[f(\lambda_1 v_1 + \dots + \lambda_n v_n) \overset{\text{f lin.}}{=} f(\lambda_1 f(v_1) + \dots + \lambda_n f(v_n)) + f(\lambda_{n+1} f(v_{n+1}))\]
		sostituendo l'ipotesi induttiva e sfruttando f lineare allora:
		\[\Rightarrow \sum_{i = 1}^{n} \lambda_i f(v_i) + \lambda_{n+1} f(v_{n+1}) = \sum_{i = 1}^{n +1} \lambda_i f(v_i)\]
	\end{enumerate}
	
	\teorem{Siano $V, W$ sp.vettoriali su $\mathbb{K}$, e sia $\{v_i\}_{i \in I}$ una lista di generatori di V, siano:
	\[V \xlongrightarrow{f} W, V \xlongrightarrow{g} W\]
	due applicazioni lineari, e sia:
	\[f(v_i) = g(v_i), \forall i \in I \Rightarrow f = g\]
	}
	
	Dimostrazione, sia $v \in V$, allora esiste $\lambda_i \in \mathbb{K}, i \in I \tc v = \sum_{i \in I} \lambda_i v_i$, allora:
	\[f(v) = \sum_{i \in I} \lambda_i f(v_i)\]
	\[g(v) = \sum_{i \in I} \lambda_i f(g_i)\]
	per la proposizione precedente, ma quindi:
	\[f(v_i) = g(v_i), \forall i \in I\]
	allora per ipotesi si ha che:
	\[f(v) = g(v)\]
	
	\teorem{Siano $v_1, \dots, v_n \in V$ linearmente indipendenti, allora dati $w_1, \dots, w_n \in W$, esiste una applicazione lineare:
	\[V \xlongrightarrow{f} W, \tc f(v_i) = w_i \forall i \in \{1, \dots , n\}\]
	}
	
	Dimostrazione, assumendo che $\mathcal{B} = \{v_1, \dots, v_n\}$ sia una base di $V$, allora $V = \brackets{v_1, \dots, v_n}$, quindi dato $v \in V$ si ha $X_\mathcal{B}(v) = (x_1, \dots, x_n), v = \sum_{i = 1}^{n} x_i v_i$, si definisce allora una applicazione da $V$ a $W$:
	\[\begin{aligned}
		V & \xlongrightarrow{f} W \\
		v & \xlongrightarrow{f} \sum_{i = 1}^{n} x_i w_i
	\end{aligned}\]
	Si verifica quindi che $f$ è lineare in quanto $f(v_i) = w_i$
	
	\teorem{Sia $V \xlongrightarrow{f} W$ una applicazione lineare, allora:\[\Im{f} \subseteq W\]
	dato da:
	\[\Im(f) := \{f(v) \tc v \in V\}\]
	$\Im{f}$ è un sottospazio di W}
	Dimostrazione:
	\[\Im(f) \not = \emptyset, V \not = \emptyset\]
	
	Siano $w_1, w_2 \in \Im(f), w_1 = f(v_1), w_2 = f(v_2), v_1, v_2 \in V$, allora:
	\[w_1 + w_2 = f(v_1) + f(v_2) = f(v_1 + v_2) \in \Im(f)\]
	
	$w \in \Im(f), \lambda \in \mathbb{K}$, allora:
	\[w = f(v), v \in V\]
	\[\lambda w = \lambda f(v) \overset{\text{f. lin}}{=} f(\lambda v) \in \Im(f)\]
	
	\teorem{Sia $V \xlongrightarrow{f} W$ una applicazione lineare, allora:\[\ker(f) \subseteq f\]
	dato da:
	\[\ker(f) := \{v \in V \tc f(v) = 0_w\}\]
	$\ker(f)$ è un sottospazio di V}
	
	\teorem{Sia $V \xlongrightarrow{f} W$ una applicazione lineare, e che $V$ sia finitamente generato, allora:
	\[\dim V = \dim \ker(f) + \dim(\Im(f))\]
	Supponendo che $\dim V = \dim W$, allora $f$ è biunivoca se e solo se $\ker(f) = \{0_v\}$}
	Dimostrazione:
	
	se $f$ è biunivoca, è anche iniettiva, quindi il suo kernel avrà al massimo un solo elemento, in quanto non esistono due valori diversi che diano la stessa immagine, nello specifico lo zero di $W$.
	
	Sapendo che vale $\ker(f) = \{0_v\}$, allora per la proposizione precendente si ha che:
	\[\dim V = \dim \Im(f) \leq \dim W\]
	ma per ipotesi:
	\[\dim V = \dim W\]
	quindi:
	\[\dim \Im(f) = \dim W\]
	quindi $\Im(f) = W$, quindi f è sia suriettiva sia iniettiva, quindi biunivoca.
	
	
	
	\chapter{Matrici}
	Sia $R$ un anello, allora una matrice mxn a entrate in $R$ è una applicazione:
	\[\{1, \dots, m\} x \{1, \dots, n\} \xlongrightarrow{f} R\]
	equivalente a:
	\[\{(i, j) \tc 1 \leq i \leq m, 1 \leq j \leq n\}\]
	equivalente a:
	\[M_{mxn} (R) := \{A \text{ matrice m x n a entrare in } R\}\]
	Notazione:
	\[f(i, j) = a_{ij}\]
	denotando $f$ con $A$ e scrivendola in forma estesa, allora:
	\[A = \begin{bmatrix} a_{1, 1} \dots a_{1, n} \\
	\vdots \ddots \vdots \\
	a_{m, 1} \dots a_{m, n} \end{bmatrix}\]
	allora si rappresenta con $A^i$ la i-esa riga della matrice:
	\[A^i = \begin{bmatrix} a_{i1}, a_{i2}, \dots, a_{in} \end{bmatrix}\]
	e si rappresenta con $A_j$ la j-esa colonna della matrice:
	\[A_j = \begin{bmatrix} a_{1j} \\ a_{2j} \\ \vdots \\ a_{mj} \end{bmatrix}\]
	
	\section{Operazioni sulle matrici}
	Ricordando che $R$ è un anello, è possibile svolgere diverse operazioni sulle matrici:
	\begin{itemize}
		\item Somma tra matrici, se hanno lo stesso numero di righe e colonne:
		\[\begin{aligned}
			M_{mxn}(R) \times M_{mxn}(R) & \longrightarrow M_{mxn}(R) \\
			(A, B) & \longrightarrow C = A + B = c_{ij}
		\end{aligned}\]
		dove:
		\[c_{ij} = c_{ij} + b_{ij}\]
		
		\item Prodotto per scalare
		\[\begin{aligned}
			R \times M_{mxn}(R) & \longrightarrow M_{mxn}(R) \\
			(\lambda, B) & \longrightarrow C = \lambda A = c_{ij}
		\end{aligned}\]
		dove:
		\[c_{ij} = \lambda a_{ij}\]	
	\end{itemize}
	Le proprietà che definiscono uno spazio vettoriale sono soddisfatte d queste operazioni, la dimostrazione delle proprietà è analoga a quella degli spazi vettoriali.
	
	Se $R$ è un campo, quindi $R = \mathbb{K}$, allora $M_{mxn}(\mathbb{K})$ è uno spazio vettoriale su $\mathbb{K}$, in cui:
	\[\dim M_{mxn}(\mathbb{K}) = m x n\]
	e la base standard è:
	\[
	E_{11} =
	\begin{bmatrix}
		1 & 0 & 0 & \cdots & 0 \\
		0 & 0 & 0 & \cdots & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0 & 0 & 0 & \cdots & 0
	\end{bmatrix}, \quad
	E_{12} =
	\begin{bmatrix}
		0 & 1 & 0 & \cdots & 0 \\
		0 & 0 & 0 & \cdots & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0 & 0 & 0 & \cdots & 0
	\end{bmatrix}, \cdots, 
	\]
	\[
	E_{ij} =
	\begin{bmatrix}
		0 & \cdots & 0 & \cdots & 0 \\
		\vdots & \ddots & \vdots & & \vdots \\
		0 & \cdots & 1 & \cdots & 0 \\
		\vdots & & \vdots & \ddots & \vdots \\
		0 & \cdots & 0 & \cdots & 0
	\end{bmatrix}, \cdots ,  \quad
	E_{mn} =
	\begin{bmatrix}
		0 & 0 & 0 & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots & \vdots \\
		1 & 0 & \cdots & 0 & 0 \\
		0 & 0 & \cdots & 0 & 1
	\end{bmatrix}
	\]
	
	\begin{itemize}
		\item Prodotto, se la prima matrice ha il numero di colonne uguale al numero di righe delle seconda matrice:
		\[\begin{aligned}
			M_{mxn}(R) \times M_{nxp}(R) & \longrightarrow M_{mxp}(R) \\
			(A, B) & \longrightarrow C
		\end{aligned}\]
		dove:
		\[c_{ij} = \sum_{t = 1}^{n} a_{it} \cdot b_{tj}\]
		ad esempio:
		\[
		A =
		\begin{bmatrix}
			2 & -1 \\
			2 & 3
		\end{bmatrix},
		\qquad
		B =
		\begin{bmatrix}
			1 & 1 & 3 \\
			2 & 0 & -1
		\end{bmatrix}.
		\]
		
		\[
		A B =
		\begin{bmatrix}
			2 & -1 \\
			2 & 3
		\end{bmatrix}
		\begin{bmatrix}
			1 & 1 & 3 \\
			2 & 0 & -1
		\end{bmatrix} = \]\[
		=
		\begin{bmatrix}
			(2)(1) + (-1)(2) & (2)(1) + (-1)(0) & (2)(3) + (-1)(-1) \\
			(2)(1) + (3)(2) & (2)(1) + (3)(0) & (2)(3) + (3)(-1)
		\end{bmatrix}
		=
		\begin{bmatrix}
			0 & 2 & 7 \\
			8 & 2 & 3
		\end{bmatrix}.
		\]
		Il prodotto tra matrici non è commutativo, in quanto frequentemente se si può fare $A \cdot B$, non si può fare $B \cdot A$, a meno che non siano matrici quadrate, e anche se si potesse fare non darebbero lo stesso risultato tranne in casi particolari.	
	\end{itemize}
	
	\subsection{Tipologie di matrici}
	\begin{itemize}
		\item Matrice unitaria, ovvero la matrice che ha come entrate nella diagonale tutti 1, ed il resto tutti 0:
		\[
		M_{n,n} = (\delta_{ij})_{1 \leq i \leq n, 1 \leq j \leq n} = 1_n
		\begin{bmatrix}
			1 & 0 & 0 & \cdots & 0 \\
			0 & 1 & 0 & \cdots & 0 \\
			0 & 0 & 1 & \ddots & 0 \\
			\vdots & \vdots & \ddots & \ddots & \vdots \\
			0 & 0 & 0 & \cdots & 1
		\end{bmatrix}
		\]
		Dove:
		\[(\delta_{ij})_{1 \leq i \leq n, 1 \leq j \leq n}\]
		è definito come il simbolo di Kronecker:
		\[(\delta_{ij})_{1 \leq i \leq n, 1 \leq j \leq n} = \left\{\begin{aligned}
			1, & i = j \\
			0, & i \not = j
		\end{aligned}\right.\]
		\item Una matrice $A \in M_{n,n}(R)$ è scalare se:
		\[A = \lambda \delta_{ij} =
		\begin{bmatrix}
			\lambda & 0 & 0 & \cdots & 0 \\
			0 & \lambda & 0 & \cdots & 0 \\
			0 & 0 & \lambda & \ddots & 0 \\
			\vdots & \vdots & \ddots & \ddots & \vdots \\
			0 & 0 & 0 & \cdots & \lambda
		\end{bmatrix}
		\]
		\item Una matrice $A \in M_{n,n}(R)$ è diagonale se: 
		\[A = \lambda_{ij} \delta_{ij} =
		\begin{bmatrix}
			\lambda_1 & 0 & 0 & \cdots & 0 \\
			0 & \lambda_2 & 0 & \cdots & 0 \\
			0 & 0 & \lambda_3 & \ddots & 0 \\
			\vdots & \vdots & \ddots & \ddots & \vdots \\
			0 & 0 & 0 & \cdots & \lambda_n
		\end{bmatrix}
		\]
	\end{itemize}
	
	\teorem{
		\begin{enumerate}
			\item $(\lambda 1_m) \cdot A = \lambda A = A\cdot(\lambda 1_n), \forall A \in M_{m,n}(R)$
			\item $(A \cdot B ) \cdot C = A \cdot (B \cdot C)$ se $A$ è $mxn$, $B$ è $nxp$, $C$ è $pxq$
			\item $A(B \cdot B^{'}) = A \cdot B + A \cdot B^{'}$, e $(B + B^{'})C = BC + B^{'}C$, se A è mxn, B e B' sono nxp, C è pxq 
		\end{enumerate}
	}
	
	Preso $\mathbb{K}$ un campo, ed una matrice $A \in M_{m,n}(\mathbb{K})$
	\[\begin{aligned}
		M_{n,1}(\mathbb{K}) = \mathbb{K}^n & \xlongrightarrow{L_A} \mathbb{K}^m M_{1,m}(\mathbb{K}) \\
		X & \longrightarrow (A \cdot X)_{m,1} 
	\end{aligned}\]
	ad esempio, presa la matrice:
	\[A = \begin{bmatrix} 2 & 1 & 1 \\ 0 & -1 & 3 \end{bmatrix} \in M_{2,3}\]
	Allora:
	\[\begin{aligned}
		\mathbb{K}^3 & \xlongrightarrow{L_A} \mathbb{K}^2\\
		X & \longrightarrow A \cdot X
	\end{aligned}\]
	Si ha che:
	\[L_A(1,1,2) = \begin{bmatrix} 2 & 1 & 1 \\ 0 & -1 & 3 \end{bmatrix} \cdot \begin{bmatrix} 1 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 5 & 5 \end{bmatrix} = (5, 5)\]
	In generale:	
	\[L_A(x_1,x_2,x_3) = \begin{bmatrix} 2 & 1 & 1 \\ 0 & -1 & 3 \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 2x_1 + x_2 + x_3 & -x_2 + x_3 \end{bmatrix} = (2x_1 + x_2 + x_3, -x_2 + x_3)\]
	
	Ora verifico che:
	\[L_A(X + Y) \overset{?}{=} L_A(X) + L_A(Y)\]
	\[\Rightarrow A(X + Y) = AX + AY\]
	ed è vero per la proprietà associativa del prodotto.
	Controllo che sia verificato anche il prodotto per scalare:
	\[L_A(\lambda Y) \overset{?}{=} \lambda L_A(Y)\]
	\[\Rightarrow A \cdot \lambda Y = \lambda A Y\]
	per la proprietà commutativa del prodotto, perchè ci troviamo in un campo
	
	\teorem{Sia $\mathbb{K}^n \xlongrightarrow{f} \mathbb{K}^m$ una applicazione lineare, allora:
	\[\exists A \in M_{m,n}(\mathbb{K}) \tc f = L_A\]
	e tale matrice è unica.}
	Per la dimostrazione bisogna sapere che presa la base standard di $\mathbb{K}^n$, allora:
	\[L_A(e_j) = A_j\]
	Dimostrazione della proposizione precedente:
	\[f = L_A = L_B\]
	allora:
	\[f(e_j) = L_A(e_j) = L_B(e_j) \Rightarrow A_j = B_j, \Rightarrow \forall 1 \leq j \leq n \rightarrow A = B\]
	quindi se esiste $A$ è unica.
	
	Esistenza di $A$: sia $A \in M_{m,n}(\mathbb{K}) \tc$
	\[A_j = f(e_j), \forall j \in \{1, \cdots, n\}\]
	poiché $f$ e $L_A$, assumono gli stessi valori su $\{e_1, \cdots, e_n\}$, ed entrambe generano $\mathbb{K}$ sono uguali.
		
	\chapter{Teoremi/Principi/Assiomi/Altro usati negli esercizi}
	\section{Principio di Dirichlet}
	\label{sec: Dirichlet}
	Afferma che:
	\begin{center}
		\teorem{Se $n + 1$ oggetti, vengono distribuiti in $n$ cassetti, allora almeno un cassetto deve contenere due oggetti}
	\end{center}
	Esiste anche la versione generalizzata:
	\begin{center}
		\teorem{Se si hanno $m$ oggetti da distribuire in $k$ cassetti, allora almeno un cassetto conterrà almeno $\lceil\frac{m}{k}\rceil$ oggetti}
	\end{center}
	dove $\lceil \rceil$ rappresenta la parte intera superiore.
	
\end{document}